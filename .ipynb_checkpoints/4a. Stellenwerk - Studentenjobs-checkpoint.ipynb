{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5c0cf6",
   "metadata": {},
   "source": [
    "# Webscraper für die Studentenjobs auf www.stellenwerk-hamburg.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac34fb7",
   "metadata": {},
   "source": [
    "## Installieren und importieren benötigter Bibliotheken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf5bb5c",
   "metadata": {},
   "source": [
    "Die Python-Bibliotheken *requests*, *BeautifulSoup* und *tqdm* müssen vor der ersten Verwendung installiert werden. Mit *requests* ist es möglich, über Python HTTP-Anfragen zu stellen. Mit *BeautifulSoup* kann man HTML-Code parsen und mit *tqdm* kann man sich bei langwierigen Programmausführungen Fortschrittsbalken anzeigen lassen. Die Installation geht folgendermaßen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0905b460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2020.12.5)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Collecting bs4\n",
      "  Using cached bs4-0.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (4.59.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install bs4\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a2366",
   "metadata": {},
   "source": [
    "Die ebenfalls benötigten Bibliotheken *time* (zum Verzögern der Programmausführung) und *csv* (zum Arbeiten mit csv-Dateien) sind Teil von Python und müssen nicht extra installiert werden. Sie müssen aber importiert werden, damit man sie verwenden kann.<br>\n",
    "Alle benötigten Bibliotheken importieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99867df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b85e5",
   "metadata": {},
   "source": [
    "## Anforderungen an den Web Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c517a1cd",
   "metadata": {},
   "source": [
    "Die **Stellenanzeigen für Studentenjobs** auf den Seiten des Stellenwerks sind [hier](https://www.stellenwerk-hamburg.de/jobboerse/unternehmen/studentenjobs) zu finden.\n",
    "Die Seite ist so aufgebaut, dass immer 20 Stellenangebote auf einer Seite verlinkt werden. Um die nächsten 20 Links zu Stellenangebote zu sehen, muss weitergeblättert werden. Die eigentliche Information zur Stelle findet sich hinter dem jeweiligen Link. Um an die Informationen über die Stellenangebote zu gelangen, müssen entsprechend diese Job-spezifischen Links aufgerufen werden.\n",
    "Der Webscraper für das Stellenwerk muss somit:\n",
    "- Webseite des Stellenwerks aufrufen\n",
    "- blättern und wissen, wie oft weitergeblättert werden muss (Seitenzahl)\n",
    "- die 20 Links je Seite auslesen\n",
    "- eine Liste aller (Seitenzahl x 20) Links zusammenstellen\n",
    "- alle so gewonnen Links zu den jeweiligen Stellenanzeigen aufrufen\n",
    "- die stellenbezogenen Informationen extrahieren\n",
    "- die extrahierten Informationen in einer CSV-Datei abspeichern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c24106b",
   "metadata": {},
   "source": [
    "## Aufbau der URL des Stellenwerks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe44070",
   "metadata": {},
   "source": [
    "Wenn man durch die Seiten mit den Stellenanzeigen blättert, stellt man fest, dass die URL des Stellenwerks [*Query Strings*](https://de.wikipedia.org/wiki/Query-String) nutzt, um Parameter (insbesondere der Suchanfrage) an den Webserver zu senden. Die URLs haben folgenden beispielhaften Aufbau:\n",
    "\n",
    "https://www.stellenwerk-hamburg.de/jobboerse/unternehmen/studentenjobs?keywords=&offer_type=job_ad_b2b&job_category=111&employment_type=All&page=1\n",
    "\n",
    "Der Query-String wird mit einem Fragezeichen (?) eingeleitet und besteht aus einem oder mehreren Schlüssel=Wert-Paaren. Im Fall mehrerer Schlüssel=Wert-Paare sind diese mit einem *&* verbunden. Vor dem Query-String steht die Basis-URL. Somit läst sich der URL des Stellenwerks in folgende Bestandteile gliedern:\n",
    "\n",
    "- Basis-URL: https://www.stellenwerk-hamburg.de/jobboerse/unternehmen/studentenjobs\n",
    "- Query-String: [keywords=&offer_type=job_ad_b2b&job_category=111&employment_type=All&page=1](https://www.stellenwerk-hamburg.de/jobboerse/unternehmen/studentenjobs?keywords=&offer_type=job_ad_b2b&job_category=111&employment_type=All&page=1)<br>\n",
    "mit folgenden Schlüssel=Wert-Paaren:\n",
    "    - keywords=\n",
    "    - offer_type=job_ad_b2b\n",
    "    - job_category=111\n",
    "    - employment_type=All\n",
    "    - page=1\n",
    "\n",
    "Diese Schlüssel=Wert-Paare des Query Strings werden im Folgenden in einem [Python-Dictionary](https://www.w3schools.com/python/python_dictionaries.asp) mit den Namen *query* gespeichert. Das Python-Dictionary wird dann - neben der Basis-URL - an *requests* übergeben, um die Webseite aufzurufen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289257cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.stellenwerk-hamburg.de/jobboerse/unternehmen/studentenjobs\"\n",
    "query = {\n",
    "    'keywords': '',\n",
    "    'offer_type': 'job_ad_b2b',\n",
    "    'job_category': '111',\n",
    "    'employment_type': 'All',\n",
    "    'page': '0'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf615dcf",
   "metadata": {},
   "source": [
    "## Wir tarnen uns als Firefox "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375a0c19",
   "metadata": {},
   "source": [
    "Standardmäßig offenbart sich die *requests*-Bibliothek selbst, indem sie sich beim Aufrufen einer Webseite im *Request Header* als User-Agent nennt: **'User-Agent': 'python-requests/2.26.0'**.<br>\n",
    "Um nicht sofort als Web-Scraper aufzufallen, ändern wir den entsprechenden Eintrag im *Request Header* und geben uns als Firefox-Browser aus (die Angaben, die Firefox sendet, habe ich mit den *Werkzeugen für Webentwickler* im Firefox-Browser ermittelt über **Netzwerkanalyse** &rightarrow; **Kopfzeilen** &rightarrow; **Anfragekopfzeilen**). Dazu werden die Angaben, die Firefox sendet, in einem Python-Dictionary namens *my_headers* gespeichert, damit sie an *requests* übergeben werden können. Damit werden die ursprünglichen Angaben zum User-Agenten überschrieben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b33bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ee31d",
   "metadata": {},
   "source": [
    "## Aufruf der Webseite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50d5030",
   "metadata": {},
   "source": [
    "Nun wird ein *Get-Request* zum Aufruf der Stellenwerk-Webseite an den Webserver gestellt. Neben der Basis-URL müssen auch die Informationen aus dem Query-String an *requests* übergeben werden. Dazu wird das oben erstellte Python-Dictionary mit dem Namen *query* dem funktionseigenen Argument *params* übergeben. Ebenso wird das Dictionary mit dem Namen *my_headers* dem funktionseigenen Argument *headers* übergeben, um uns so als Firefox-Browser zu tarnen.<br>\n",
    "Anfrage und die Antwort des Webservers werden von *requests* in einem sog. Response-Objekt gespeichert. Dieses Response-Objekt wird dann der [Variablen](https://www.w3schools.com/python/python_variables.asp) *r* zugewiesen, so dass wir im weiteren Verlauf damit arbeiten können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ab1e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(base_url, params = query, headers = my_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77f346a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.stellenwerk-hamburg.de/jobboerse/unternehmen/studentenjobs?keywords=&offer_type=job_ad_b2b&job_category=111&employment_type=All&page=0\n"
     ]
    }
   ],
   "source": [
    "print(r.request.url) # aufgerufene URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed299034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*', 'Connection': 'keep-alive'}\n"
     ]
    }
   ],
   "source": [
    "print(r.request.headers) # an den Webserver gesendete Anfragekopfezeile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dca152b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "print(r.status_code) # War der Webseitenaufruf erfolgreich?\n",
    "print(r.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5319c2a",
   "metadata": {},
   "source": [
    "## HTML-Code der Webseite speichern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54195edd",
   "metadata": {},
   "source": [
    "Nun wird mit `.text` der übermittelte HTML-Quellcode der Stellenwerk-Webseite als Unicode-Text aus dem Response-Objekt *r* extrahiert und in der Variable *html* gespeichert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ed87063",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83038bcd",
   "metadata": {},
   "source": [
    "Diese Variable wird als Input an *BeautifulSoup* übergeben und von *BeautifulSoup* in ein *BeautifulSoup*-Objekt umgewandelt. *BeautifulSoup*-Objekte erlauben es, durch den HTML-Quellcode zu navigieren, in diesem Code nach spezifischen Inhalten zu suchen und diese Inhalte zu extrahieren. Das *BeautifulSoup*-Objekt wird der Variable *bs* zugewiesen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be089ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3379dff8",
   "metadata": {},
   "source": [
    "## Gesuchte Informationen aus dem HTML-Code auslesen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db076a",
   "metadata": {},
   "source": [
    "### Links zu den jeweiligen Stellenanzeigen auslesen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3c6d07",
   "metadata": {},
   "source": [
    "Aus dem Quellcode der Webseite wird das HTML-Element ausgelesen, in dem die Links zu den Stellenanzeigen zu finden sind. Um dieses Element im Quellcode zu identifizieren, verwendet man die *Browserwerkzeuge* bzw. *Entwicklertools* des jeweiligen Browsers. Diese kann man in den meisten Browsern mit <kbd>Strg</kbd>+<kbd>UMSCHALTTASTE</kbd>+<kbd>I</kbd> öffnen. Hier wählt man dann den Reiter *Inspektor*.\n",
    "Wenn man nun die Maus über die Webseite bewegt, ändert sich das hervorgehobene Element. Gleichzeitig wird der dazugehörge HTML-Quellcode im Fensterbereich der *Browserwerkzeuge* angezeigt. Hier sind auch rudimentäre Kenntnisse von HTML und CSS hilfreich, um die entsprechenden Elemente im Quellcode zu bestimmen.<br>\n",
    "Die Links zu den ausgeschriebenen Jobs befinden sich in einem\n",
    "[*div*](https://www.w3schools.com/tags/tag_div.ASP)-Tag, welches mit einem CSS [*class*](https://www.w3schools.com/css/css_selectors.asp)-Selektor gestaltet wurde. In diesem Fall ist *class=\"JobSearch-content\"*. Mit diesen Informationen und der Funktion  `.find()` von *BeautifulSoup* können die entsprechenden Informationen aus dem HTML-Quellcode extrahiert und in eine Variable *job_ads* gespeichert werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8051937",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ads = bs.find('div', class_ = 'JobSearch-content')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a8a0ab",
   "metadata": {},
   "source": [
    "Aus diesem so extrahierten HTML-Element werden in einem weiteren Schritt alle Links zu den Stellenanzeigen ausgelesen. Links werden in HTML über einen [*a*](https://www.w3schools.com/tags/tag_a.asp)-Tag definiert. Die Links zu den Stellenanzeigen befinden sich in einem *a*-Tag mit dem CSS-Selektor *class=\"Link\"*. Da es hier mehrere entsprechende Elemente gibt, muss die Funktion `.find_all()` von *BeautifulSoup* verwendet werden, um alle Links zu finden. Die so extrahierten Links werden in die Listen-Variable *job_links* geschrieben. Die URL, zu der der Link führt, befindet sich in dem *href*-Attribut des *a*-Tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd2585e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_links = job_ads.find_all('a', class_ = 'Link')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d21674f",
   "metadata": {},
   "source": [
    "Anzahl an Elementen (hier: Links zu den Stellenanzeigen) in der Liste *job_links*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d325b7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(job_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dca624",
   "metadata": {},
   "source": [
    "Es befinden sich immer 20 Links zu den Stellenanzeigen auf jeder der Unterseiten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4257c19",
   "metadata": {},
   "source": [
    "Als nächstes werfen wir einen Blick auf die so extrahierten Links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faf0c238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/jobboerse/praktikant-werkstudent-mwd-globales-hr-management-hamburg-211209-507382\n",
      "/jobboerse/werkstudent-recruiting-mwd-hamburg-211209-507377\n",
      "/jobboerse/werkstudent-dmw-im-bereich-technisches-facility-management-hamburg-211209-507353\n",
      "/jobboerse/betreuerin-familienanaloge-wohngruppe-hamburg-211209-507352\n",
      "/jobboerse/werkstudentin-customer-success-hamburg-211209-507351\n",
      "/jobboerse/betreuerin-familienanaloge-wohngruppe-hamburg-211209-507349\n",
      "/jobboerse/werkstudent-it-mwd-hamburg-211209-507345\n",
      "/jobboerse/werkstudent-praktikant-wmd-corporate-development-finance-hamburg-211209-507337\n",
      "/jobboerse/werkstudentin-compliance-hamburg-211209-507336\n",
      "/jobboerse/student-assistant-middle-east-mfd-editorial-research-intelligence-hamburg-211209-507334\n",
      "/jobboerse/student-assistant-asia-pacific-mfd-editorial-research-intelligence-hamburg-211209-507333\n",
      "/jobboerse/student-assistant-japan-mfd-editorial-research-intelligence-hamburg-211209-507332\n",
      "/jobboerse/werkstudent-praktikant-wmd-hr-administration-hamburg-211209-507330\n",
      "/jobboerse/student-assistant-china-mfd-editorial-research-intelligence-hamburg-211209-507327\n",
      "/jobboerse/student-assistant-fluent-french-mfd-hamburg-211209-507326\n",
      "/jobboerse/werkstudentin-strategic-data-partnerships-mwd-hamburg-211209-507325\n",
      "/jobboerse/working-student-quality-management-supply-chain-mfd-hamburg-211209-507323\n",
      "/jobboerse/werkstudent-wmx-rezeptabrechnung-sportmedizin-hamburg-211209-507317\n",
      "/jobboerse/werkstudent-im-online-marketing-mwd-mind-2-tagewo-hamburg-211209-507315\n",
      "/jobboerse/werkstudent-mwd-gesetzliche-krankenversicherungen-hamburg-211124-504775\n"
     ]
    }
   ],
   "source": [
    "for l in job_links:\n",
    "    print(l['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a3f1f4",
   "metadata": {},
   "source": [
    "Wie man sieht, handelt es sich um relative statt [absolute Links](https://de.wikipedia.org/wiki/Absoluter_Link). Die fehlenden Informationen - Protokoll (https://) und Hostname (www.stellenwerk-hamburg.de) - werden den Links vorangestellt und die nun vollständigen Links werden in der Listen-Variable *rows* gespeichert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98a6635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for link in job_links:\n",
    "    link = 'https://www.stellenwerk-hamburg.de' + link['href']\n",
    "    rows.append(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae255854",
   "metadata": {},
   "source": [
    "Jetzt sehen die Links so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a49680b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.stellenwerk-hamburg.de/jobboerse/praktikant-werkstudent-mwd-globales-hr-management-hamburg-211209-507382\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/werkstudent-recruiting-mwd-hamburg-211209-507377\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/werkstudent-dmw-im-bereich-technisches-facility-management-hamburg-211209-507353\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/betreuerin-familienanaloge-wohngruppe-hamburg-211209-507352\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/werkstudentin-customer-success-hamburg-211209-507351\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/betreuerin-familienanaloge-wohngruppe-hamburg-211209-507349\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/werkstudent-it-mwd-hamburg-211209-507345\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/werkstudent-praktikant-wmd-corporate-development-finance-hamburg-211209-507337\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/werkstudentin-compliance-hamburg-211209-507336\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/student-assistant-middle-east-mfd-editorial-research-intelligence-hamburg-211209-507334\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/student-assistant-asia-pacific-mfd-editorial-research-intelligence-hamburg-211209-507333\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/student-assistant-japan-mfd-editorial-research-intelligence-hamburg-211209-507332\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/werkstudent-praktikant-wmd-hr-administration-hamburg-211209-507330\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/student-assistant-china-mfd-editorial-research-intelligence-hamburg-211209-507327\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/student-assistant-fluent-french-mfd-hamburg-211209-507326\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/werkstudentin-strategic-data-partnerships-mwd-hamburg-211209-507325\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/working-student-quality-management-supply-chain-mfd-hamburg-211209-507323\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/werkstudent-wmx-rezeptabrechnung-sportmedizin-hamburg-211209-507317\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/werkstudent-im-online-marketing-mwd-mind-2-tagewo-hamburg-211209-507315\n",
      "https://www.stellenwerk-hamburg.de/jobboerse/werkstudent-mwd-gesetzliche-krankenversicherungen-hamburg-211124-504775\n"
     ]
    }
   ],
   "source": [
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5892d6d8",
   "metadata": {},
   "source": [
    "### Blättern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bad0f20",
   "metadata": {},
   "source": [
    "Die Stellenanzeigen sind über mehrere Seiten à 20 Links verteilt. Wir müssen also blättern, um an alle Links zu kommen. Wie oft geblättert werden muss, kann man der Seite im unteren Bereich entnehmen.<br>\n",
    "Die Information zur Anzahl der Seiten befindet sich in einem [*span*](https://www.w3schools.com/tags/tag_span.asp)-Tag mit dem CSS-Selektor *class=\"Pagination-text\"*. Mit der Funktion  `.find()` von *BeautifulSoup* kann die Informationen aus dem HTML-Quellcode extrahiert werden. Mit der Funktion `.get_text()` wird die so extrahierte Information in Text umgewandelt (andernfalls wäre sie ein *BeautifulSoup*-Objekt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1473158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seite 1 von 45\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "pages = bs.find('span', class_ = 'Pagination-text').get_text()\n",
    "print(pages)\n",
    "print(type(pages)) # Abfrage des Datentyps; hier: string (Zeichenkette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae077925",
   "metadata": {},
   "source": [
    "Aus diesem String kann man die maximale Seitenzahl folgendermaßen isolieren. Der String wird über die Methode `.split()` am Leerzeichen (' ') in seine Komponenten zerlegt. Aus der Liste der Komponenten wird dann das vierte Element extrahiert. Das vierte Element hat den Index 3, da die Indexzählung bei 0 beginnt. Schließlich wird der String mit `int()` in ein Integer (ganzzahliger Wert) umgewandelt und ist somit numerisch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3a35884",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_max = int(pages.split(' ')[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16f51b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(page_max)\n",
    "print(type(page_max)) # Abfrage des Datentyps; hier: integer (ganzzahliger Wert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a968c",
   "metadata": {},
   "source": [
    "Nun kann diese Information genutzt werden, um zu blättern. Dazu wird eine *for*-Schleife in Kombination mit der Funktion `range()` verwendet. `range()` bestimmt, wie häufig die Schleife durchlaufen wird. Hier setzen wir *page_max* ein und addieren eine Eins hinzu, da `range()` standardmäßig den Stoppwert nicht mit einschließt (also nur bis *page_max* - 1 läuft). In jedem Schleifendurchlauf wird im *Query-String* der Wert für den Schlüssel *page* (s. oben unter Aufbau der URL des Stellenwerks) um eins erhöht. Im ersten Durchlauf ist page=0, im zweiten page=1, im dritten page=2 usw. Alle Links zu den Stellenanzeigen, die auf einer Seite zu finden sind, werden dann extrahiert und zu der Liste *all_links* hinzugefügt.<br>\n",
    "Nach jedem Schleifendurchlauf stoppen wir mit `time.sleep(1)` das Programm für 1 Sekunde, bevor wir die nächste Seite aufrufen. Andernfalls würden wir mit zu vielen Anfragen in kurzer Zeit eventuell den Server überlasten. Dies würde schlimmstenfalls eine [Denial-of-Service-Attacke](https://www.bsi.bund.de/DE/Themen/Verbraucherinnen-und-Verbraucher/Cyber-Sicherheitslage/Methoden-der-Cyber-Kriminalitaet/DoS-Denial-of-Service/dos-denial-of-service_node.html) darstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4c26ecb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddf29c5bb97424eaa847bd7067977f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_links = [] # Anlegen einer leeren Liste zur Aufnahme der Links\n",
    "for page in tqdm(range(page_max + 1)): # tqdm vor range erzeugt einen Fortschrittsbalken \n",
    "    query[\"page\"] = page # Seitenzahl wird in jedem Schleifendurchlauf um 1 erhöht\n",
    "    r = requests.get(base_url, params = query) # Webseite aufrufen\n",
    "    html = r.text # HTML-Code der Webseite speichern\n",
    "    bs = BeautifulSoup(html, 'html.parser') # HTML-Code an BeautifulSoup übergeben\n",
    "    job_ads = bs.find('div', class_ = 'JobSearch-content') # Element mit Links zu den Stellenanzeigen extrahieren\n",
    "    job_links = job_ads.find_all('a', class_ = 'Link') # aus dieser Spalte alle Links zu den Anzeigen extrahieren\n",
    "    for link in job_links: # Protokoll und Hostname jedem Link voranstellen\n",
    "        link = 'https://www.stellenwerk-hamburg.de' + link['href']\n",
    "        all_links.append(link) # Links der Liste all_links hinzufügen\n",
    "    time.sleep(1) # eine Sekunde Verzögerung nach jedem Seitenaufruf (friendly scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9bd18",
   "metadata": {},
   "source": [
    "Wie viele Elemente hat die Liste *all_links* nach dem durch alle Seiten geblättert wurde? Diese Anzahl entspricht der Anzahl an Stellenangeboten zum Scraping-Zeitpunkt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95bdb082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "893\n"
     ]
    }
   ],
   "source": [
    "print(len(all_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0dbbb4",
   "metadata": {},
   "source": [
    "Nun liegt eine Liste mit allen Links zu den Stellenangeboten vor. Jetzt muss noch bestimmt werden, welche Informationen zu den jeweiligen Stellenangeboten extrahiert werden sollen. Dazu ruft man beispielhaft eine oder mehrere Stellenanzeigen aus der Liste *all_links* auf untersucht den HTML-Quellcode mit den Entwicklerwerkzeugen. So können alle HTML-Elemente identifiziert werden, in denen die Job-spezifischen Informationen zu finden sind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97738b",
   "metadata": {},
   "source": [
    "### Allgemeine Informationen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f9d826",
   "metadata": {},
   "source": [
    "Allgemeinere Informationen wie Job-Bezeichnung, Datum und ID der Anzeige sowie der Anzeigetyp finden sich im oberen Bereich der Seite und lassen sich wie folgt aus dem Quellcode herauslesen. Bei *ad_date* wird die Zeichenkette \"Online seit DD.MM.YYYY\" mit Hilfe der Methode `.split()` am Leerzeichen (' ') in seine Komponenten zerlegt und dann das dritte Element (DD.MM.YYYY) extrahiert. \n",
    "\n",
    "\n",
    "Hier kommt die Funktion `.select()` von *BeautifulSoup* zum Einsatz, der man einen [CSS-Selektor](https://www.w3schools.com/cssref/css_selectors.asp) übergeben kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "efd27370",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title = bs.find('h1', class_ = 'JobHead-title').get_text()\n",
    "ad_date = bs.find('li', class_ = 'JobHead-additionalInfoItem').get_text().split(' ')[2]\n",
    "ad_id = bs.select('li.JobHead-additionalInfoItem:nth-child(2)')[0].get_text()\n",
    "ad_type = bs.select('li.JobHead-additionalInfoItem:nth-child(3)')[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a6f49",
   "metadata": {},
   "source": [
    "### Anzeigendaten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c88728f",
   "metadata": {},
   "source": [
    "Informationen wie Art und Zeitraum der Beschäftigung, Vergütung sowie Bewerbungslink oder Bewerbungs-E-Mail (es gibt nur eins von beiden) finden sich im unteren Drittel der Stellenanzeigen.<br>\n",
    "Hier werden das erste Mal mit [*try* und *except*](https://www.w3schools.com/python/python_try_except.asp) Ausnahmen behandelt:<br>\n",
    "+ Nicht jede Stellenanzeige weist eine Vergütung aus\n",
    "+ Wenn ein Bewerbunslink angegeben wurde, fehlt die Bewerbungs-E-Mail und vice versa, da es auf der Webseite des Stellenwerks sich ausschließende Alternativen sind.\n",
    "\n",
    "Ein Exraktionsversuch bei fehlenden Informationen würde zu einer Fehlermeldung und zu einem Programmabbruch führen. Für so einen Fall wird im *except*-Block explizit der Wert *None* vergeben. Durch diese Konstruktion wird ein Programmabbruch verhindert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "76b8c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_type = bs.find(string = \"Art der Beschäftigung\").find_next('dd').get_text() # Suche nach dem Tag mit Inhalt \"Art der Beschäftigung\", dann das folgende dd-Tag wählen\n",
    "job_duration = bs.find(string = \"Zeitraum der Beschäftigung\").find_next('dd').get_text()\n",
    "try:\n",
    "    job_wage = bs.find(string = \"Vergütung\").find_next('dd').get_text()\n",
    "except:\n",
    "    job_wage = None\n",
    "try:\n",
    "    app_link = bs.find(string = \"Bewerbungslink\").find_next('dd').find_next('a', class_ = 'Link')['href']\n",
    "except:\n",
    "    app_link = None\n",
    "try:\n",
    "    app_email = bs.find(string = \"Bewerbungs-E-Mail\").find_next('a', class_ = 'IconText-mail IconText')['href']\n",
    "except:\n",
    "    app_email = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce9bf3d",
   "metadata": {},
   "source": [
    "### Beschreibung und Anforderungsprofil der Stelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0044e9c",
   "metadata": {},
   "source": [
    "Die Beschreibung der Stelle und das Anforderungsprofil sind die zentralen stellenbezogenen Informationen und finden sich in zwei seperaten Textfeldern im mittleren Teil der Seite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "853d6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_descrip = bs.find_all('div', class_ = 'RichText')[0].get_text()\n",
    "job_profile = bs.find_all('div', class_ = 'RichText')[1].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ba6967",
   "metadata": {},
   "source": [
    "### Firmenkontaktdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab79134",
   "metadata": {},
   "source": [
    "Die Firmenkontaktdaten stehen am Ende der Seite und beinhalten Informationen wie Firmenname, Adresse und Kontaktperson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "17b60652",
   "metadata": {},
   "outputs": [],
   "source": [
    "company = bs.find(string = \"Firmenname\").find_next('dd').get_text()\n",
    "loc = bs.find(string = \"Standort\").find_all_next('dd', limit = 2)\n",
    "location = loc[0].get_text()+ \", \" + loc[1].get_text()\n",
    "contact = bs.find(string = \"Kontaktperson\").find_next('dd').get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c7f6a9",
   "metadata": {},
   "source": [
    "## Schleife über alle Stellenanzeigen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d5d7b",
   "metadata": {},
   "source": [
    "Nun wird alles kombiniert. Die Informationen (allgemeine Informationen, Anzeigendaten, Beschreibung und Anforderungsprofil der Stelle, Firmeninformationen) müssen nun für *alle* Stellenanzeigen extrahiert werden. Mit Hilfe einer *for*-Schleife werden dafür alle Links aus *all_links* nacheinander aufgerufen, der jeweilige HTML-Quellcode abgerufen, aus diesem die relevanten Informationen zu jeder Stellenanzeige extrahiert und in einem Python-Dictionary (Schlüssel-Wert-Paare) names *row* gespeichert. Diese Dictionaries werden dann ein eine Liste namens *rows* übertragen, d.h. die Liste hat so viele Elemente in Form von Dictionaries, wie es Stellenanzeigen gibt. Die *try*-und-*except*-Blöcke verhindern den Programm-Abbruch falls in einzelnen Stellenanzeigen manche Informationen nicht vorhanden sind und somit nicht ausgelesen werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a2e6ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c482e393317441669d55e7cee6e4efb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/893 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = []\n",
    "for link in tqdm(all_links): # tqdm vor der iterierbaren Liste 'all_links' erzeugt einen Fortschrittsbalken \n",
    "    r = requests.get(link, headers = my_headers) # Webseite aufrufen\n",
    "    html = r.text # HTML-Code der Webseite speichern\n",
    "    bs = BeautifulSoup(html, 'html.parser') # HTML-Code an BeautifulSoup übergeben\n",
    "    try: # alle benötigen Informationen aus dem HTML-Code extrahieren\n",
    "        job_title = bs.find('h1', class_ = 'JobHead-title').get_text()\n",
    "    except:\n",
    "        job_title = None\n",
    "    try:\n",
    "        ad_date = bs.find('li', class_ = 'JobHead-additionalInfoItem').get_text().split(' ')[2]\n",
    "    except:\n",
    "        ad_date = None\n",
    "    try:    \n",
    "        ad_id = bs.select('li.JobHead-additionalInfoItem:nth-child(2)')[0].get_text()\n",
    "    except:\n",
    "        ad_id = None\n",
    "    try:\n",
    "        ad_type = bs.select('li.JobHead-additionalInfoItem:nth-child(3)')[0].get_text()\n",
    "    except:\n",
    "        ad_type = None\n",
    "    try:\n",
    "        job_type = bs.find(string = \"Art der Beschäftigung\").find_next('dd').get_text() # Suche nach dem Tag mit Inhalt \"Art der Beschäftigung\", dann das folgende dd-Tag wählen\n",
    "    except:\n",
    "        job_type = None\n",
    "    try:\n",
    "        job_duration = bs.find(string = \"Zeitraum der Beschäftigung\").find_next('dd').get_text()\n",
    "    except:\n",
    "        job_duration = None\n",
    "    try:\n",
    "        job_wage = bs.find(string = \"Vergütung\").find_next('dd').get_text()\n",
    "    except:\n",
    "        job_wage = None\n",
    "    try:\n",
    "        app_link = bs.find(string = \"Bewerbungslink\").find_next('dd').find_next('a', class_ = 'Link')['href']\n",
    "    except:\n",
    "        app_link = None\n",
    "    try:\n",
    "        app_email = bs.find(string = \"Bewerbungs-E-Mail\").find_next('a', class_ = 'IconText-mail IconText')['href']\n",
    "    except:\n",
    "        app_email = None\n",
    "    try:\n",
    "        company = bs.find(string = \"Firmenname\").find_next('dd').get_text()\n",
    "    except:\n",
    "        company = None\n",
    "    try:\n",
    "        loc = bs.find(string = \"Standort\").find_all_next('dd', limit = 2)\n",
    "        location = loc[0].get_text()+ \", \" + loc[1].get_text()\n",
    "    except:\n",
    "        loc, location = None, None\n",
    "    try:\n",
    "        contact = bs.find(string = \"Kontaktperson\").find_next('dd').get_text()\n",
    "    except:\n",
    "        contact = None\n",
    "    try:\n",
    "        job_descrip = bs.find_all('div', class_ = 'RichText')[0].get_text()\n",
    "    except:\n",
    "        job_descrip = None\n",
    "    try:\n",
    "        job_profile = bs.find_all('div', class_ = 'RichText')[1].get_text()\n",
    "    except:\n",
    "        job_profile = None\n",
    "    row = { # alle extrahierten Informationen in ein Dictionary schreiben\n",
    "    'job_title': job_title,\n",
    "    'ad_date': ad_date,\n",
    "    'ad_id': ad_id,\n",
    "    'ad_type': ad_type,\n",
    "    'job_type': job_type,\n",
    "    'job_duration': job_duration,\n",
    "    'job_wage': job_wage,\n",
    "    'app_link': app_link,\n",
    "    'app_email': app_email,\n",
    "    'company': company,\n",
    "    'location': location,\n",
    "    'contact': contact,\n",
    "    'job_descrip': job_descrip,\n",
    "    'job_profile': job_profile,\n",
    "    'link': link\n",
    "    }\n",
    "    rows.append(row) # Dictionary names row wird der Liste namens rows angehängt\n",
    "    time.sleep(1) # eine Sekunde Verzögerung nach jedem Seitenaufruf (friendly scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6bef10",
   "metadata": {},
   "source": [
    "## Extrahierte Daten in CSV-Datei schreiben"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b377203",
   "metadata": {},
   "source": [
    "Die extrahierten Daten, die in der Liste *rows* gespeichert sind, werden abschließend in eine rechteckige Datenmatrix geschrieben. Jede Stellenanzeige (Merkmalsträger) bildet eine Zeile. Die Merkmale der Stellenanzeigen bilden die Spalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8b5f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'daten/job_ads.csv'\n",
    "with open(file, mode = 'w', encoding = 'utf-8', newline = '') as f:\n",
    "    col_names = ['link', 'job_title', 'ad_date', 'ad_id', 'ad_type', 'job_type', 'job_duration', 'job_wage', 'app_link', 'app_email', 'company', 'location', 'contact', 'job_descrip', 'job_profile', 'link']\n",
    "    writer = csv.DictWriter(f, fieldnames = col_names)\n",
    "    writer.writeheader()\n",
    "    for row in rows:\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
