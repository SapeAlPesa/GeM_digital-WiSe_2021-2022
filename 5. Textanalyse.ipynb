{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "416bd836",
   "metadata": {},
   "source": [
    "# Automatisierte Textanalyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7112b2b8",
   "metadata": {},
   "source": [
    "Nachdem mittels Webscraping der Textkorpus für die Analyse erstellt wurde, müssen diese Informationen nun verarbeitet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9806aeb1",
   "metadata": {},
   "source": [
    "Alle für die Textanalyse benötigten Bibliotheken installieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44aa8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a673256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk # nltk = natural language processing tool kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd0ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd91f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f20df12",
   "metadata": {},
   "source": [
    "Englisches Sprachpaket installieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "029fafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dbe971",
   "metadata": {},
   "source": [
    "Deutsches Sprachpaket installieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8053fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54b8f78",
   "metadata": {},
   "source": [
    "Englisches Sprachpaket aktivieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cffde2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "560d9978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea011f1",
   "metadata": {},
   "source": [
    "nltk-Datensätze aus dem Internet laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc4cfc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c4b95c",
   "metadata": {},
   "source": [
    "Beispieltext wird der Variable doc zugewiesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b5b1ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I've been 2 times to New York in 2011, but did not have the constitution for it. It DIDN'T appeal to me. I preferred Los Angeles.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16d8ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower() # alles in Kleinbuchstaben umwandeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dbc10a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e00f95b",
   "metadata": {},
   "source": [
    "Für die automatisierte Textverarbeitung müssen Texte häufig vorverarbeitet werden. Texte sind ersteinmal sehr lange Zeichenketten. Zur Strukturierung können diese Zeichenketten in einzelne Sätze oder einzelne Wörter zerlegt werden.\n",
    "<dl>\n",
    "<dt>Sentence splitting</dt>\n",
    "<dd>Ein Dokument in einzelne Sätze zerlegen und Liste der einzelnen Sätze erstellt.</dd>\n",
    "\n",
    "<dt>Tokenisierung</dt>\n",
    "<dd>Satz in seine Bestandteile zerlegen.</dd>\n",
    "</dl>\n",
    "Mit der Tokenisierung wird eine Liste (bzw. bei satzweiser Tokenisierung mehrere Listen) von Untersuchungseinheiten (Wörter) erstellt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e0dc09",
   "metadata": {},
   "source": [
    "Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6f29090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i've been 2 times to new york in 2011, but did not have the constitution for it.\n",
      "it didn't appeal to me.\n",
      "i preferred los angeles.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689cd1da",
   "metadata": {},
   "source": [
    "In den folgenden Code-Beispielen wird häufig [List Comprehension](https://www.w3schools.com/python/python_lists_comprehension.asp) eingesetzt. *List Comprehension* ist eine elegante Möglichkeit, Python-Listen zu erzeugen. Ein hilfreiches Erklärvideo zu diesem Konzept findet sich [hier](https://www.youtube.com/watch?v=3dt4OGnU5sM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e192e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sentence.text for sentence in doc.sents] # Sätze in Anführungszeichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "575e0a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i've been 2 times to new york in 2011, but did not have the constitution for it.\", \"it didn't appeal to me.\", 'i preferred los angeles.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ca26d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sents = [sent for sent in doc.sents] # Sätze ohne Anführungszeichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25ca11b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i've been 2 times to new york in 2011, but did not have the constitution for it., it didn't appeal to me., i preferred los angeles.]\n"
     ]
    }
   ],
   "source": [
    "print(doc_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce19a2",
   "metadata": {},
   "source": [
    "Tokenisierung für das gesamte Dokument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "887a0a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.text for token in doc] # ohne .text werden Wörter ohne Anführungzeichen aufgelistet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ae348c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'ve\", 'been', '2', 'times', 'to', 'new', 'york', 'in', '2011', ',', 'but', 'did', 'not', 'have', 'the', 'constitution', 'for', 'it', '.', 'it', 'did', \"n't\", 'appeal', 'to', 'me', '.', 'i', 'preferred', 'los', 'angeles', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e734ef1",
   "metadata": {},
   "source": [
    "Satzweise Tokenisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e411eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [[token.text for token in sentence] for sentence in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f125fe63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', \"'ve\", 'been', '2', 'times', 'to', 'new', 'york', 'in', '2011', ',', 'but', 'did', 'not', 'have', 'the', 'constitution', 'for', 'it', '.'], ['it', 'did', \"n't\", 'appeal', 'to', 'me', '.'], ['i', 'preferred', 'los', 'angeles', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2ac98a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'ve\", 'been', '2', 'times', 'to', 'new', 'york', 'in', '2011', ',', 'but', 'did', 'not', 'have', 'the', 'constitution', 'for', 'it', '.']\n",
      "\n",
      "['it', 'did', \"n't\", 'appeal', 'to', 'me', '.']\n",
      "\n",
      "['i', 'preferred', 'los', 'angeles', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in tokens:\n",
    "    print(t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c2449",
   "metadata": {},
   "source": [
    "Bedeutungsgleiche Wörter kommen häufig in verschiedenen Varianten vor. Die einzelnen Wörter können auch vereinfacht werden, indem sie auf eine gemeinsame Grundform oder einen gemeinsamen Wortstamm reduziert werden. Somit lässt sich die Anzahl von Untersuchungseinheiten (Wörtern) gleicher Bedeutung reduzieren.\n",
    "\n",
    "Hier gibt es grundsätzlich zwei Möglichkeiten, auf die man Variationen von Wörtern reduzieren kann:\n",
    "- [Lemma](https://de.wikipedia.org/wiki/Lemma_(Lexikographie)) - Die Grundform des Wortes, wie sie in einem Wörterbuch steht. Z.B.: Haus, laufen, begründen\n",
    "\n",
    "- [Stamm](https://de.wikipedia.org/wiki/Wortstamm) - Das Wort ohne Flexionsendungen (Prefixe und Suffixe). Z.B.: Haus, lauf, begründ\n",
    "\n",
    "<dl>\n",
    "<dt>Lemmatisierung</dt>\n",
    "<dd>Jede Variation eines Wortes in Bezug auf Zeit oder Menge wird unter zur Hilfenahme eines elektronsichen Wörterbuches duch die Grundform ersetzt.</dd>\n",
    "<dt>Stemming</dt>\n",
    "<dd>Durch regelbasiertes Abschneiden oder Ersetzen von Suffixen entsteht ein künstlicher Wortstamm. Dieser Vorgang beruht ausschließlich auf einem Algorithmus ohne zur Hilfenahme eines Wörterbuchs.</dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8fa64f",
   "metadata": {},
   "source": [
    "Lemmatisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d386ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [[token.lemma_ for token in sentence] for sentence in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3207e2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', \"'ve\", 'be', '2', 'time', 'to', 'new', 'york', 'in', '2011', ',', 'but', 'do', 'not', 'have', 'the', 'constitution', 'for', 'it', '.'], ['it', 'do', \"n't\", 'appeal', 'to', 'I', '.'], ['I', 'prefer', 'los', 'angeles', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7639fe",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d698d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71131c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f72f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = [[stemmer.stem(token) for token in sentence] for sentence in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5bfabf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 've', 'been', '2', 'time', 'to', 'new', 'york', 'in', '2011', ',', 'but', 'did', 'not', 'have', 'the', 'constitut', 'for', 'it', '.'], ['it', 'did', \"n't\", 'appeal', 'to', 'me', '.'], ['i', 'prefer', 'los', 'angel', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d3dcda",
   "metadata": {},
   "source": [
    "Wenn nicht nur einzelne Wörter sondern auch den unmitterbaren Kontext untersuchen möchte, kann man ein gleitendes Fenster von *n* Wörtern verwenden, um den Text zu untersuchen. Ein derartiges Fenster wird *n-gram* genannt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb28d2f",
   "metadata": {},
   "source": [
    "n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a74194e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cee9c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [gram for gram in ngrams(tokens[0], 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1903ed41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', \"'ve\"), (\"'ve\", 'been'), ('been', '2'), ('2', 'times'), ('times', 'to'), ('to', 'new'), ('new', 'york'), ('york', 'in'), ('in', '2011'), ('2011', ','), (',', 'but'), ('but', 'did'), ('did', 'not'), ('not', 'have'), ('have', 'the'), ('the', 'constitution'), ('constitution', 'for'), ('for', 'it'), ('it', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4cbdc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = [gram for gram in ngrams(tokens[0], 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbc9b0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', \"'ve\", 'been'), (\"'ve\", 'been', '2'), ('been', '2', 'times'), ('2', 'times', 'to'), ('times', 'to', 'new'), ('to', 'new', 'york'), ('new', 'york', 'in'), ('york', 'in', '2011'), ('in', '2011', ','), ('2011', ',', 'but'), (',', 'but', 'did'), ('but', 'did', 'not'), ('did', 'not', 'have'), ('not', 'have', 'the'), ('have', 'the', 'constitution'), ('the', 'constitution', 'for'), ('constitution', 'for', 'it'), ('for', 'it', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c23dbf",
   "metadata": {},
   "source": [
    "## Wortartenerkennung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a3e046",
   "metadata": {},
   "source": [
    "Man kann die einzelnen Wörter eines Textes verschiedenen [Wortarten](https://de.wikipedia.org/wiki/Wortart) zuordnen. Im Englischen werden die Wortarten *parts of speech* (POS) genannt. Die automatische Zuordnung von Wörtern und Satzzeichen eines Textes zu Wortarten firmiert entsprechend unter [part of speech bzw. POS tagging](https://de.wikipedia.org/wiki/Part-of-speech-Tagging)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e070b97d",
   "metadata": {},
   "source": [
    "Folgende Wortarten werden in *spacy*'s POS tagger unterschieden:\n",
    "\n",
    "| POS   |        DESCRIPTION        |\n",
    "|:------|:--------------------------|\n",
    "| ADJ   | adjective                 |\n",
    "| ADP   | adposition                |\n",
    "| ADV   | adverb                    |\n",
    "| AUX   | auxiliary                 |\n",
    "| CONJ  | conjunction               |\n",
    "| CCONJ | coordinating conjunction  |\n",
    "| DET   | determiner                |\n",
    "| INTJ  | interjection              |\n",
    "| NOUN  | noun                      |\n",
    "| NUM   | numeral                   |\n",
    "| PART  | particle                  |\n",
    "| PRON  | pronoun                   |\n",
    "| PROPN | proper noun               |\n",
    "| PUNCT | punctuation               |\n",
    "| SCONJ | subordinating conjunction |\n",
    "| SYM   | symbol                    |\n",
    "| VERB  | verb                      |\n",
    "| X     | other                     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "987e34e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [token.pos_ for token in doc] # Wichtig: Tokenisierung muss schon durchgeführt worden sein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23a9db23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'AUX', 'VERB', 'NUM', 'NOUN', 'ADP', 'PROPN', 'PROPN', 'ADP', 'NUM', 'PUNCT', 'CCONJ', 'AUX', 'PART', 'VERB', 'DET', 'NOUN', 'ADP', 'PRON', 'PUNCT', 'PRON', 'AUX', 'PART', 'VERB', 'ADP', 'PRON', 'PUNCT', 'PRON', 'VERB', 'PROPN', 'PROPN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3105dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sents = [[token.pos_ for token in sentence] for sentence in doc.sents] # satzweise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2461bfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['PRON', 'AUX', 'VERB', 'NUM', 'NOUN', 'ADP', 'PROPN', 'PROPN', 'ADP', 'NUM', 'PUNCT', 'CCONJ', 'AUX', 'PART', 'VERB', 'DET', 'NOUN', 'ADP', 'PRON', 'PUNCT'], ['PRON', 'AUX', 'PART', 'VERB', 'ADP', 'PRON', 'PUNCT'], ['PRON', 'VERB', 'PROPN', 'PROPN', 'PUNCT']]\n"
     ]
    }
   ],
   "source": [
    "print(pos_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f61c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pos_tuples = list(zip(words, pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7663407b",
   "metadata": {},
   "source": [
    "Die beiden Listen *words* und *pos* werden zu einer Liste von Tupeln kombiniert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2b432e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'PRON'),\n",
       " (\"'ve\", 'AUX'),\n",
       " ('been', 'VERB'),\n",
       " ('2', 'NUM'),\n",
       " ('times', 'NOUN'),\n",
       " ('to', 'ADP'),\n",
       " ('new', 'PROPN'),\n",
       " ('york', 'PROPN'),\n",
       " ('in', 'ADP'),\n",
       " ('2011', 'NUM'),\n",
       " (',', 'PUNCT'),\n",
       " ('but', 'CCONJ'),\n",
       " ('did', 'AUX'),\n",
       " ('not', 'PART'),\n",
       " ('have', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('constitution', 'NOUN'),\n",
       " ('for', 'ADP'),\n",
       " ('it', 'PRON'),\n",
       " ('.', 'PUNCT'),\n",
       " ('it', 'PRON'),\n",
       " ('did', 'AUX'),\n",
       " (\"n't\", 'PART'),\n",
       " ('appeal', 'VERB'),\n",
       " ('to', 'ADP'),\n",
       " ('me', 'PRON'),\n",
       " ('.', 'PUNCT'),\n",
       " ('i', 'PRON'),\n",
       " ('preferred', 'VERB'),\n",
       " ('los', 'PROPN'),\n",
       " ('angeles', 'PROPN'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pos_tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d54187f",
   "metadata": {},
   "source": [
    "## Stoppwörter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdacfac",
   "metadata": {},
   "source": [
    "[Stoppwörter](https://de.wikipedia.org/wiki/Stoppwort) sind Wörter, die ausschließlich grammatikalische/syntaktische Funktionen im Satz übernehmen und somit keine Bedeutung für die Erfassung des Dokumentinhalts besitzen, wie etwa: \n",
    "- bestimmte Artikel (der, die, das)\n",
    "- unbestimmte Artikel (einer, eine)\n",
    "- Konjunktionen (und, oder, doch, weil, ...)\n",
    "- Präpositionen (an, in, von, ...)\n",
    "\n",
    "Aufgrund ihrer inhaltlichen Bedeutungslosigkeit und der großen Häufigkeit, mit der sie in Texten vorkommen, ist es meist sinnvoll, sie bei der Textanalyse zu ignorieren.\n",
    "Der Ausschluss von Stoppwörtern erfolgt über die Wortart (s.o.) oder über Stoppwort-Listen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a981e4",
   "metadata": {},
   "source": [
    "Ausschluss von Stoppwörtern aus dem Beispieltext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3afc517",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [[token.text for token in sentence if token.pos_ in {'NOUN', 'VERB', 'PROPN', 'ADJ', 'ADV'} and not token.is_stop] for sentence in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e71e10a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['times', 'new', 'york', 'constitution'], ['appeal'], ['preferred', 'los', 'angeles']]\n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d588e7bb",
   "metadata": {},
   "source": [
    "## Eigennamenerkennung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b7648b",
   "metadata": {},
   "source": [
    "In Texten kommen auch [Eigennamen](https://de.wikipedia.org/wiki/Eigenname) vor. Eigennamenerkennung (Named-entity recognition, NER) von spacy kann - zumindest im Englischen - folgende Kategorien unterscheiden:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa55287",
   "metadata": {},
   "source": [
    "| Label       | Beschreibung                                                |\n",
    "|:------------|:------------------------------------------------------------|\n",
    "| PERSON      | Person                                                      |\n",
    "| NORP        | Nationality Or Religious or Political group                 |\n",
    "| FAC         | facility                                                    |\n",
    "| ORG         | organisation                                                |\n",
    "| GPE         | GeoPolitical Entity                                         |\n",
    "| LOC         | locations (wie etwa Seen oder Gebirge)                      |\n",
    "| PRODUCT     | Produkt                                                     |\n",
    "| EVENT       | Ereignis (etwa im Sport, in der Politik, in der Geschichte) |\n",
    "| WORK_OF_ART | Kunstwerk                                                   |\n",
    "| LAW         | Gesetz                                                      |\n",
    "| LANGUAGE    | Sprache                                                     |\n",
    "| DATE        | Datum                                                       |\n",
    "| TIME        | Zeitangabe                                                  |\n",
    "| PERCENT     | Prozentwert                                                 |\n",
    "| MONEY       | Geldbetrag                                                  |\n",
    "| QUANTITY    | Häufigkeit                                                  |\n",
    "| ORDINAL     | Ordinalzahl (erste, zweite, dritte, ...)                    |\n",
    "| CARDINAL    | Kardinalzahl (1, 2, 3, ...)                                 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de2a1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [[(entity.text, entity.label_) for entity in nlp(sentence.text).ents] for sentence in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aca4ad6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('2', 'CARDINAL'), ('new york', 'GPE'), ('2011', 'DATE')], [], [('los angeles', 'GPE')]]\n"
     ]
    }
   ],
   "source": [
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7deac8",
   "metadata": {},
   "source": [
    "Grafische Darstellung der Eigennamen im Satzzusammenhang:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c00abbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">i've been \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " times to \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    new york\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2011\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", but did not have the constitution for it. it didn't appeal to me. i preferred \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    los angeles\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy.displacy.render(doc, style = \"ent\", jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1618ba50",
   "metadata": {},
   "source": [
    "## Syntaktische Analyse des Textes (Dependenzgrammatik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b29b0c1",
   "metadata": {},
   "source": [
    "Die inhaltliche Bedeutung eines Wortes im Satzzusammenhang kann nicht immer über die Wortart (POS) bestimmt werden. Sie hängt auch von der grammatischen Funktion des Worts im Satz ab. Die [Dependenzgrammatik](https://de.wikipedia.org/wiki/Dependenzgrammatik) analysiert die Beziehungen der Wörter untereinander durch ihre Beziehung zum Hauptverb des jeweiligen Satzes. Folgende syntaktische Kategorien werden unterschieden:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e83777",
   "metadata": {},
   "source": [
    "| Label      | Beschreibung                                     |\n",
    "|:-----------|:-------------------------------------------------|\n",
    "| acl        | clausal modifier of a noun (adjectival clause)   |\n",
    "| advcl      | adverbial clause modifier                        |\n",
    "| advmod     | adverbial modifier                               |\n",
    "| amod       | adjectival modifier                              |\n",
    "| appos      | appositional modifier                            |\n",
    "| aux        | auxiliary verb                                   |\n",
    "| case       | case marker                                      |\n",
    "| cc         | coordinating conjunction                         |\n",
    "| ccomp      | clausal complement                               |\n",
    "| clf        | classifier                                       |\n",
    "| compound   | compound                                         |\n",
    "| conj       | conjunction                                      |\n",
    "| cop        | copula                                           |\n",
    "| csubj      | clausal subject                                  |\n",
    "| dep        | unspecified dependency                           |\n",
    "| det        | determiner                                       |\n",
    "| discourse  | discourse element                                |\n",
    "| dislocated | dislocated elements                              |\n",
    "| dobj       | direct object                                    |\n",
    "| expl       | expletive                                        |\n",
    "| fixed      | fixed multiword expression                       |\n",
    "| flat       | flat multiword expression                        |\n",
    "| goeswith   | goes with                                        |\n",
    "| iobj       | indirect object                                  |\n",
    "| list       | list                                             |\n",
    "| mark       | marker                                           |\n",
    "| nmod       | nominal modifier                                 |\n",
    "| nsubj      | nominal subject                                  |\n",
    "| nummod     | numeric modifier                                 |\n",
    "| obj        | object                                           |\n",
    "| obl        | oblique nominal                                  |\n",
    "| orphan     | orphan                                           |\n",
    "| parataxis  | parataxis                                        |\n",
    "| pobj       | prepositional object                             |\n",
    "| punct      | punctuation                                      |\n",
    "| reparandum | overridden disfluency                            |\n",
    "| root       | the root of the sentence, usually a verb         |\n",
    "| vocative   | vocative                                         |\n",
    "| xcomp      | open clausal complement                          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c659db4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('i', 'been', 'nsubj'),\n",
       "  (\"'ve\", 'been', 'aux'),\n",
       "  ('been', 'been', 'ROOT'),\n",
       "  ('2', 'times', 'nummod'),\n",
       "  ('times', 'been', 'attr'),\n",
       "  ('to', 'been', 'prep'),\n",
       "  ('new', 'york', 'compound'),\n",
       "  ('york', 'to', 'pobj'),\n",
       "  ('in', 'been', 'prep'),\n",
       "  ('2011', 'in', 'pobj'),\n",
       "  (',', 'been', 'punct'),\n",
       "  ('but', 'been', 'cc'),\n",
       "  ('did', 'have', 'aux'),\n",
       "  ('not', 'have', 'neg'),\n",
       "  ('have', 'been', 'conj'),\n",
       "  ('the', 'constitution', 'det'),\n",
       "  ('constitution', 'have', 'dobj'),\n",
       "  ('for', 'constitution', 'prep'),\n",
       "  ('it', 'for', 'pobj'),\n",
       "  ('.', 'been', 'punct')],\n",
       " [('it', 'appeal', 'nsubj'),\n",
       "  ('did', 'appeal', 'aux'),\n",
       "  (\"n't\", 'appeal', 'neg'),\n",
       "  ('appeal', 'appeal', 'ROOT'),\n",
       "  ('to', 'appeal', 'prep'),\n",
       "  ('me', 'to', 'pobj'),\n",
       "  ('.', 'appeal', 'punct')],\n",
       " [('i', 'preferred', 'nsubj'),\n",
       "  ('preferred', 'preferred', 'ROOT'),\n",
       "  ('los', 'angeles', 'compound'),\n",
       "  ('angeles', 'preferred', 'dobj'),\n",
       "  ('.', 'preferred', 'punct')]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(c.text, c.head.text, c.dep_) for c in nlp(sentence.text)] for sentence in doc.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db653130",
   "metadata": {},
   "source": [
    "Grafische Darstellung der syntaktischen Abhängigkeiten im Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bfae3ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"e4413e6847af42fa9971d0f2a5e6c1bb-0\" class=\"displacy\" width=\"4950\" height=\"662.0\" direction=\"ltr\" style=\"max-width: none; height: 662.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">i</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">'ve</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">been</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">2</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">times</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">new</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">york</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">2011,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">but</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">did</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">not</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">have</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">constitution</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">it.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">it</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">did</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">n't</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">appeal</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3900\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3900\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4075\">me.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4075\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4250\">i</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4250\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4425\">preferred</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4425\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4600\">los</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4600\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4775\">angeles.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4775\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-0\" stroke-width=\"2px\" d=\"M70,527.0 C70,352.0 380.0,352.0 380.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,529.0 L62,517.0 78,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-1\" stroke-width=\"2px\" d=\"M245,527.0 C245,439.5 375.0,439.5 375.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,529.0 L237,517.0 253,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-2\" stroke-width=\"2px\" d=\"M595,527.0 C595,439.5 725.0,439.5 725.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,529.0 L587,517.0 603,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-3\" stroke-width=\"2px\" d=\"M420,527.0 C420,352.0 730.0,352.0 730.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M730.0,529.0 L738.0,517.0 722.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-4\" stroke-width=\"2px\" d=\"M420,527.0 C420,264.5 910.0,264.5 910.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M910.0,529.0 L918.0,517.0 902.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-5\" stroke-width=\"2px\" d=\"M1120,527.0 C1120,439.5 1250.0,439.5 1250.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,529.0 L1112,517.0 1128,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-6\" stroke-width=\"2px\" d=\"M945,527.0 C945,352.0 1255.0,352.0 1255.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1255.0,529.0 L1263.0,517.0 1247.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-7\" stroke-width=\"2px\" d=\"M420,527.0 C420,177.0 1440.0,177.0 1440.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1440.0,529.0 L1448.0,517.0 1432.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-8\" stroke-width=\"2px\" d=\"M1470,527.0 C1470,439.5 1600.0,439.5 1600.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1600.0,529.0 L1608.0,517.0 1592.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-9\" stroke-width=\"2px\" d=\"M420,527.0 C420,89.5 1795.0,89.5 1795.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1795.0,529.0 L1803.0,517.0 1787.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-10\" stroke-width=\"2px\" d=\"M1995,527.0 C1995,352.0 2305.0,352.0 2305.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1995,529.0 L1987,517.0 2003,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-11\" stroke-width=\"2px\" d=\"M2170,527.0 C2170,439.5 2300.0,439.5 2300.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">neg</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2170,529.0 L2162,517.0 2178,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-12\" stroke-width=\"2px\" d=\"M420,527.0 C420,2.0 2325.0,2.0 2325.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2325.0,529.0 L2333.0,517.0 2317.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-13\" stroke-width=\"2px\" d=\"M2520,527.0 C2520,439.5 2650.0,439.5 2650.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2520,529.0 L2512,517.0 2528,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-14\" stroke-width=\"2px\" d=\"M2345,527.0 C2345,352.0 2655.0,352.0 2655.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2655.0,529.0 L2663.0,517.0 2647.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-15\" stroke-width=\"2px\" d=\"M2695,527.0 C2695,439.5 2825.0,439.5 2825.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2825.0,529.0 L2833.0,517.0 2817.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-16\" stroke-width=\"2px\" d=\"M2870,527.0 C2870,439.5 3000.0,439.5 3000.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3000.0,529.0 L3008.0,517.0 2992.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-17\" stroke-width=\"2px\" d=\"M3220,527.0 C3220,264.5 3710.0,264.5 3710.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3220,529.0 L3212,517.0 3228,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-18\" stroke-width=\"2px\" d=\"M3395,527.0 C3395,352.0 3705.0,352.0 3705.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3395,529.0 L3387,517.0 3403,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-19\" stroke-width=\"2px\" d=\"M3570,527.0 C3570,439.5 3700.0,439.5 3700.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">neg</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3570,529.0 L3562,517.0 3578,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-20\" stroke-width=\"2px\" d=\"M3745,527.0 C3745,439.5 3875.0,439.5 3875.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3875.0,529.0 L3883.0,517.0 3867.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-21\" stroke-width=\"2px\" d=\"M3920,527.0 C3920,439.5 4050.0,439.5 4050.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-21\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4050.0,529.0 L4058.0,517.0 4042.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-22\" stroke-width=\"2px\" d=\"M4270,527.0 C4270,439.5 4400.0,439.5 4400.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-22\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4270,529.0 L4262,517.0 4278,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-23\" stroke-width=\"2px\" d=\"M4620,527.0 C4620,439.5 4750.0,439.5 4750.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-23\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4620,529.0 L4612,517.0 4628,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-24\" stroke-width=\"2px\" d=\"M4445,527.0 C4445,352.0 4755.0,352.0 4755.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e4413e6847af42fa9971d0f2a5e6c1bb-0-24\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4755.0,529.0 L4763.0,517.0 4747.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\", jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e7c77",
   "metadata": {},
   "source": [
    "## Reguläre Ausdrücke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "384d4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a5fe089",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45b3095d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(1, 3), match='at'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(re.search(pattern, 'later'))\n",
    "print(re.match(pattern, 'later'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f179a76",
   "metadata": {},
   "source": [
    "## Pointwise Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "24cc2d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.book import text1 as moby_dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cdbbb0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "86e3967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = moby_dick\n",
    "words = words = [word.lower() for document in documents for word in document.split() if len(word) > 2 and word not in stopwords_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e4576d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('moby', 'dick'): 84.0,\n",
       " ('sperm', 'whale'): 20.152729120423608,\n",
       " ('mrs', 'hussey'): 9.941176470588236,\n",
       " ('mast', 'heads'): 4.565801334054444,\n",
       " ('sag', 'harbor'): 4.0,\n",
       " ('vinegar', 'cruet'): 4.0,\n",
       " ('dough', 'boy'): 3.7067873303167422,\n",
       " ('try', 'works'): 3.4838709677419355,\n",
       " ('caw', 'caw'): 3.4722222222222223,\n",
       " ('white', 'whale'): 3.457170557261702,\n",
       " ('cape', 'horn'): 3.4133333333333336,\n",
       " ('new', 'bedford'): 3.272727272727273,\n",
       " ('quarter', 'deck'): 3.2339339991315676,\n",
       " ('deacon', 'deuteronomy'): 3.2,\n",
       " ('edmund', 'burke'): 3.0,\n",
       " ('father', 'mapple'): 3.0,\n",
       " ('gamy', 'jesty'): 3.0,\n",
       " ('hoky', 'poky'): 3.0,\n",
       " ('jesty', 'joky'): 3.0,\n",
       " ('joky', 'hoky'): 3.0,\n",
       " ('sperma', 'ceti'): 3.0,\n",
       " ('sporty', 'gamy'): 3.0,\n",
       " ('sulk', 'pout'): 3.0,\n",
       " ('twos', 'threes'): 3.0,\n",
       " ('samuel', 'enderby'): 2.8051948051948052,\n",
       " ('mast', 'head'): 2.6435007302550275,\n",
       " ('000', 'lbs'): 2.45,\n",
       " ('chief', 'mate'): 2.3465617516250425,\n",
       " ('straits', 'sunda'): 2.25,\n",
       " ('old', 'man'): 2.2409487666034154,\n",
       " ('crow', 'nest'): 2.227272727272727,\n",
       " ('crested', 'comb'): 2.0,\n",
       " ('daboll', 'arithmetic'): 2.0,\n",
       " ('distension', 'contraction'): 2.0,\n",
       " ('gemini', 'twins'): 2.0,\n",
       " ('helter', 'skelter'): 2.0,\n",
       " ('hogs', 'bristles'): 2.0,\n",
       " ('kith', 'kin'): 2.0,\n",
       " ('lirra', 'skirra'): 2.0,\n",
       " ('pell', 'mell'): 2.0,\n",
       " ('rio', 'plata'): 2.0,\n",
       " ('ruinous', 'discount'): 2.0,\n",
       " ('sagittarius', 'archer'): 2.0,\n",
       " ('sprinkling', 'mistifying'): 2.0,\n",
       " ('squaw', 'tistig'): 2.0,\n",
       " ('tolland', 'county'): 2.0,\n",
       " ('veriest', 'trifles'): 2.0,\n",
       " ('heidelburgh', 'tun'): 1.9285714285714286,\n",
       " ('seams', 'dents'): 1.8285714285714285,\n",
       " ('aunt', 'charity'): 1.7777777777777777,\n",
       " ('lamp', 'feeder'): 1.6896551724137931,\n",
       " ('years', 'ago'): 1.6875,\n",
       " ('latitude', 'longitude'): 1.6025641025641026,\n",
       " ('don', 'sebastian'): 1.5802469135802468,\n",
       " ('ding', 'dong'): 1.5,\n",
       " ('dong', 'ding'): 1.5,\n",
       " ('pre', 'adamite'): 1.5,\n",
       " ('ross', 'browne'): 1.5,\n",
       " ('captain', 'ahab'): 1.4873214806178956,\n",
       " ('warp', 'woof'): 1.3736263736263736,\n",
       " ('aye', 'aye'): 1.3639125910509886,\n",
       " ('captain', 'peleg'): 1.3459295161422822,\n",
       " ('550', 'ankers'): 1.3333333333333333,\n",
       " ('cretan', 'labyrinth'): 1.3333333333333333,\n",
       " ('davy', 'jones'): 1.3333333333333333,\n",
       " ('kee', 'hee'): 1.3333333333333333,\n",
       " ('renegades', 'castaways'): 1.3333333333333333,\n",
       " ('similitude', 'ceases'): 1.3333333333333333,\n",
       " ('texel', 'leyden'): 1.3333333333333333,\n",
       " ('iii', 'duodecimo'): 1.3020833333333333,\n",
       " ('funny', 'sporty'): 1.2857142857142858,\n",
       " ('chapter', 'the'): 1.2819669461857852,\n",
       " ('jack', 'knife'): 1.1797235023041475,\n",
       " ('frederick', 'cuvier'): 1.1636363636363636,\n",
       " ('marchant', 'service'): 1.1636363636363636,\n",
       " ('whosoever', 'raises'): 1.125,\n",
       " ('spouter', 'inn'): 1.1160714285714286,\n",
       " ('pivot', 'hole'): 1.1136363636363635,\n",
       " ('lower', 'jaw'): 1.103448275862069,\n",
       " ('seventy', 'seventh'): 1.0909090909090908,\n",
       " ('absent', 'minded'): 1.0666666666666667,\n",
       " ('right', 'whale'): 1.0003619156682475,\n",
       " ('!\\'\"', 'wharton'): 1.0,\n",
       " ('1729', '\"...'): 1.0,\n",
       " ('1750', '1788'): 1.0,\n",
       " ('1840', 'october'): 1.0,\n",
       " ('abstained', 'patrolling'): 1.0,\n",
       " ('abstract', 'unfractioned'): 1.0,\n",
       " ('accidental', 'advantages'): 1.0,\n",
       " ('accountants', 'computed'): 1.0,\n",
       " ('achilles', 'shield'): 1.0,\n",
       " ('administered', 'vernacular'): 1.0,\n",
       " ('adoring', 'cherubim'): 1.0,\n",
       " ('afflicted', 'jaundice'): 1.0,\n",
       " ('affluent', 'cultivated'): 1.0,\n",
       " ('agassiz', 'imagines'): 1.0,\n",
       " ('agonies', 'endures'): 1.0,\n",
       " ('agrarian', 'freebooting'): 1.0,\n",
       " ('aides', 'marshals'): 1.0,\n",
       " ('alb', 'tunic'): 1.0,\n",
       " ('albert', 'durer'): 1.0,\n",
       " ('alexanders', 'parcelling'): 1.0,\n",
       " ('allurings', 'girlish'): 1.0,\n",
       " ('amend', 'reports'): 1.0,\n",
       " ('amidst', 'rustiness'): 1.0,\n",
       " ('amounts', 'butchering'): 1.0,\n",
       " ('amphitheatrical', 'heights'): 1.0,\n",
       " ('anacharsis', 'clootz'): 1.0,\n",
       " ('ancestry', 'posterity'): 1.0,\n",
       " ('andrew', 'jackson'): 1.0,\n",
       " ('animate', 'inanimate'): 1.0,\n",
       " ('annawon', 'headmost'): 1.0,\n",
       " ('anno', '1652'): 1.0,\n",
       " ('annus', 'mirabilis'): 1.0,\n",
       " ('antemosaic', 'unsourced'): 1.0,\n",
       " ('anti', 'scorbutic'): 1.0,\n",
       " ('anus', 'breasts'): 1.0,\n",
       " ('append', 'initials'): 1.0,\n",
       " ('approve', 'omnisciently'): 1.0,\n",
       " ('approvingly', 'coax'): 1.0,\n",
       " ('argo', 'navis'): 1.0,\n",
       " ('arkansas', 'duellist'): 1.0,\n",
       " ('aroostook', 'hemlock'): 1.0,\n",
       " ('arrantest', 'topers'): 1.0,\n",
       " ('aspersion', 'disproved'): 1.0,\n",
       " ('asphaltic', 'pavement'): 1.0,\n",
       " ('asphaltites', 'unforgiven'): 1.0,\n",
       " ('assails', 'chases'): 1.0,\n",
       " ('assaulted', 'voracious'): 1.0,\n",
       " ('atrocious', 'scoundrel'): 1.0,\n",
       " ('attends', 'falconer'): 1.0,\n",
       " ('australian', 'settlement'): 1.0,\n",
       " ('authorized', 'legislative'): 1.0,\n",
       " ('availle', 'returne'): 1.0,\n",
       " ('avers', 'earl'): 1.0,\n",
       " ('baboon', 'vows'): 1.0,\n",
       " ('bakers', 'bankers'): 1.0,\n",
       " ('baleful', 'comets'): 1.0,\n",
       " ('balena', 'vero'): 1.0,\n",
       " ('baliene', 'ordinaire'): 1.0,\n",
       " ('balmy', 'autumnal'): 1.0,\n",
       " ('barest', 'ruggedest'): 1.0,\n",
       " ('barges', 'actium'): 1.0,\n",
       " ('bartholomew', 'diaz'): 1.0,\n",
       " ('bass', 'viol'): 1.0,\n",
       " ('basso', 'relievo'): 1.0,\n",
       " ('bedside', 'squatted'): 1.0,\n",
       " ('befogged', 'bedeadened'): 1.0,\n",
       " ('belated', 'innocently'): 1.0,\n",
       " ('belial', 'bondsman'): 1.0,\n",
       " ('berlin', 'gazette'): 1.0,\n",
       " ('bermudas', 'phil'): 1.0,\n",
       " ('bespattering', 'glorying'): 1.0,\n",
       " ('bess', 'gallantly'): 1.0,\n",
       " ('betty', 'snarles'): 1.0,\n",
       " ('beverage', 'circulates'): 1.0,\n",
       " ('bewildering', 'mediums'): 1.0,\n",
       " ('bigot', 'fadeless'): 1.0,\n",
       " ('blackened', 'elevations'): 1.0,\n",
       " ('boggy', 'soggy'): 1.0,\n",
       " ('bolder', 'waded'): 1.0,\n",
       " ('boldest', 'persevering'): 1.0,\n",
       " ('bombay', 'apollo'): 1.0,\n",
       " ('bombazine', 'cloak'): 1.0,\n",
       " ('bondsman', 'spurn'): 1.0,\n",
       " ('booble', 'alley'): 1.0,\n",
       " ('borean', 'dismasting'): 1.0,\n",
       " ('bountifully', 'laughable'): 1.0,\n",
       " ('bowings', 'scrapings'): 1.0,\n",
       " ('boyhood', 'thoughtless'): 1.0,\n",
       " ('braining', 'feats'): 1.0,\n",
       " ('brats', 'tinkerings'): 1.0,\n",
       " ('braves', 'mustered'): 1.0,\n",
       " ('breasts', 'extend'): 1.0,\n",
       " ('bridegroom', 'clasp'): 1.0,\n",
       " ('brides', 'benignity'): 1.0,\n",
       " ('briefly', 'exhibit'): 1.0,\n",
       " ('brindled', 'cow'): 1.0,\n",
       " ('brisson', 'marten'): 1.0,\n",
       " ('bubbled', 'seethed'): 1.0,\n",
       " ('burghers', 'calais'): 1.0,\n",
       " ('buttered', 'judgmatically'): 1.0,\n",
       " ('butteries', 'cheeseries'): 1.0,\n",
       " ('bystanders', 'zoroaster'): 1.0,\n",
       " ('cabalistics', 'keystone'): 1.0,\n",
       " ('caesarian', 'heir'): 1.0,\n",
       " ('calomel', 'jalap'): 1.0,\n",
       " ('candies', 'maccaroni'): 1.0,\n",
       " ('canny', 'seth'): 1.0,\n",
       " ('caput', 'regina'): 1.0,\n",
       " ('carcass', 'rabid'): 1.0,\n",
       " ('carey', 'chickens'): 1.0,\n",
       " ('cartloads', 'stowe'): 1.0,\n",
       " ('cats', 'purr'): 1.0,\n",
       " ('caudam', 'bracton'): 1.0,\n",
       " ('celled', 'honeycombs'): 1.0,\n",
       " ('champions', 'kingly'): 1.0,\n",
       " ('chaotic', 'bundling'): 1.0,\n",
       " ('characterized', 'partial'): 1.0,\n",
       " ('charging', 'twopence'): 1.0,\n",
       " ('cheapest', 'cheeriest'): 1.0,\n",
       " ('chemistry', 'villany'): 1.0,\n",
       " ('chewing', 'sprat'): 1.0,\n",
       " ('childe', 'harold'): 1.0,\n",
       " ('chill', 'lapsed'): 1.0,\n",
       " ('chinks', 'crannies'): 1.0,\n",
       " ('chipping', 'craters'): 1.0,\n",
       " ('chivalric', 'crusaders'): 1.0,\n",
       " ('christenings', 'whom'): 1.0,\n",
       " ('chrysalis', 'roundingly'): 1.0,\n",
       " ('circumventing', 'satirizing'): 1.0,\n",
       " ('civilly', 'moderation'): 1.0,\n",
       " ('clad', 'skins'): 1.0,\n",
       " ('clammy', 'reception'): 1.0,\n",
       " ('claus', 'pott'): 1.0,\n",
       " ('clearer', 'sweetener'): 1.0,\n",
       " ('clootz', 'deputation'): 1.0,\n",
       " ('clothe', 'doubly'): 1.0,\n",
       " ('coalescing', 'gurry'): 1.0,\n",
       " ('cocoa', 'nut'): 1.0,\n",
       " ('cohorts', 'endlessly'): 1.0,\n",
       " ('colds', 'catarrhs'): 1.0,\n",
       " ('collated', 'migrations'): 1.0,\n",
       " ('compactness', 'gloss'): 1.0,\n",
       " ('compunctions', 'suicide'): 1.0,\n",
       " ('conceited', 'ignoramus'): 1.0,\n",
       " ('conflicting', 'aptitudes'): 1.0,\n",
       " ('confound', 'remarking'): 1.0,\n",
       " ('congeal', 'eyelashes'): 1.0,\n",
       " ('conquering', 'earls'): 1.0,\n",
       " ('consecrated', 'consecrating'): 1.0,\n",
       " ('conspicuously', 'label'): 1.0,\n",
       " ('contemplating', 'amputation'): 1.0,\n",
       " ('contented', 'restricting'): 1.0,\n",
       " ('contested', 'election'): 1.0,\n",
       " ('contracts', 'thickens'): 1.0,\n",
       " ('contrastingly', 'concurred'): 1.0,\n",
       " ('contributed', 'receptive'): 1.0,\n",
       " ('controllable', 'occupant'): 1.0,\n",
       " ('convict', 'bunyan'): 1.0,\n",
       " ('copying', 'ducks'): 1.0,\n",
       " ('corinthians', 'corruption'): 1.0,\n",
       " ('corporal', 'animosity'): 1.0,\n",
       " ('countersinkers', 'superiors'): 1.0,\n",
       " ('courting', 'notoriety'): 1.0,\n",
       " ('crafty', 'upraising'): 1.0,\n",
       " ('crappoes', 'frenchmen'): 1.0,\n",
       " ('create', 'unsubduable'): 1.0,\n",
       " ('creative', 'libertines'): 1.0,\n",
       " ('crim', 'con'): 1.0,\n",
       " ('crockett', 'kit'): 1.0,\n",
       " ('cruelty', 'ganders'): 1.0,\n",
       " ('cruize', '1846'): 1.0,\n",
       " ('czarship', 'withstanding'): 1.0,\n",
       " ('dame', 'isabella'): 1.0,\n",
       " ('damsels', 'caressed'): 1.0,\n",
       " ('dauntless', 'stander'): 1.0,\n",
       " ('dauphine', 'paris'): 1.0,\n",
       " ('davenant', 'preface'): 1.0,\n",
       " ('deadening', 'filliping'): 1.0,\n",
       " ('deathful', 'whaleboat'): 1.0,\n",
       " ('debtor', 'blockhead'): 1.0,\n",
       " ('deceitfully', 'tapered'): 1.0,\n",
       " ('deceiving', 'bedevilling'): 1.0,\n",
       " ('decidedly', 'objectionable'): 1.0,\n",
       " ('decoction', 'seneca'): 1.0,\n",
       " ('defaced', 'unequal'): 1.0,\n",
       " ('deluge', 'drowns'): 1.0,\n",
       " ('denominate', 'apparatus'): 1.0,\n",
       " ('denunciations', 'forewarnings'): 1.0,\n",
       " ('deplore', 'inability'): 1.0,\n",
       " ('deriding', 'gesture'): 1.0,\n",
       " ('derision', ';--\"'): 1.0,\n",
       " ('descartian', 'vortices'): 1.0,\n",
       " ('descendants', 'unknowing'): 1.0,\n",
       " ('descriptively', 'treating'): 1.0,\n",
       " ('desolateness', 'reigning'): 1.0,\n",
       " ('detract', 'dramatically'): 1.0,\n",
       " ('deviations', 'azimuth'): 1.0,\n",
       " ('devilishness', 'desperation'): 1.0,\n",
       " ('dexterities', 'sleights'): 1.0,\n",
       " ('diaz', 'reputed'): 1.0,\n",
       " ('dig', 'dig'): 1.0,\n",
       " ('dignified', 'whitewashed'): 1.0,\n",
       " ('diligence', 'leuwenhoeck'): 1.0,\n",
       " ('diluted', 'pickled'): 1.0,\n",
       " ('discharges', 'rifles'): 1.0,\n",
       " ('disciple', 'spurzheim'): 1.0,\n",
       " ('discourse', 'parlors'): 1.0,\n",
       " ('disencumber', 'snarls'): 1.0,\n",
       " ('disgusted', 'carking'): 1.0,\n",
       " ('disjointedly', 'twiske'): 1.0,\n",
       " ('dislike', 'bitterness'): 1.0,\n",
       " ('dismasting', 'blasts'): 1.0,\n",
       " ('dismember', 'dismemberer'): 1.0,\n",
       " ('dissociated', 'characterizing'): 1.0,\n",
       " ('dissolved', 'mayst'): 1.0,\n",
       " ('distilled', 'volatile'): 1.0,\n",
       " ('drawlingly', 'soothingly'): 1.0,\n",
       " ('dreamiest', 'shadiest'): 1.0,\n",
       " ('dryden', 'annus'): 1.0,\n",
       " ('dung', 'lime'): 1.0,\n",
       " ('dut', 'ger'): 1.0,\n",
       " ('earl', 'leicester'): 1.0,\n",
       " ('eave', 'troughs'): 1.0,\n",
       " ('eccentric', 'span'): 1.0,\n",
       " ('eckermann', 'conversations'): 1.0,\n",
       " ('eddyings', 'angry'): 1.0,\n",
       " ('eleventh', 'commandment'): 1.0,\n",
       " ('elizabeth', 'oakes'): 1.0,\n",
       " ('elks', 'warringly'): 1.0,\n",
       " ('elves', 'heedlessly'): 1.0,\n",
       " ('eminent', 'tremendousness'): 1.0,\n",
       " ('eminently', 'presuming'): 1.0,\n",
       " ('employment', 'frugal'): 1.0,\n",
       " ('emprise', 'weightiest'): 1.0,\n",
       " ('enclosed', 'vitals'): 1.0,\n",
       " ('engrafted', 'clerical'): 1.0,\n",
       " ('enlightened', 'gourmand'): 1.0,\n",
       " ('entitle', 'embarks'): 1.0,\n",
       " ('ephesian', 'sod'): 1.0,\n",
       " ('erromanggoans', 'pannangians'): 1.0,\n",
       " ('esau', 'jacob'): 1.0,\n",
       " ('etherial', 'thrill'): 1.0,\n",
       " ('eventuated', 'liberation'): 1.0,\n",
       " ('evilly', 'protruding'): 1.0,\n",
       " ('evincing', 'observance'): 1.0,\n",
       " ('evoke', 'eyeless'): 1.0,\n",
       " ('exacted', 'implicit'): 1.0,\n",
       " ('excellently', 'spiralizes'): 1.0,\n",
       " ('excluding', 'suburbs'): 1.0,\n",
       " ('executor', 'legatee'): 1.0,\n",
       " ('exegetist', 'supposes'): 1.0,\n",
       " ('exegetists', 'opined'): 1.0,\n",
       " ('exercises', 'boasts'): 1.0,\n",
       " ('expediency', 'conciliating'): 1.0,\n",
       " ('exploding', 'bomb'): 1.0,\n",
       " ('exploring', 'expeditions'): 1.0,\n",
       " ('exported', 'furs'): 1.0,\n",
       " ('fac', 'similes'): 1.0,\n",
       " ('fantasy', 'sipping'): 1.0,\n",
       " ('fasting', 'humiliation'): 1.0,\n",
       " ('fata', 'morgana'): 1.0,\n",
       " ('feasted', 'fatness'): 1.0,\n",
       " ('feastest', 'bloated'): 1.0,\n",
       " ('featured', 'unbodied'): 1.0,\n",
       " ('feebly', 'pointest'): 1.0,\n",
       " ('feegeeans', 'tongatobooarrs'): 1.0,\n",
       " ('feegees', 'tramping'): 1.0,\n",
       " ('feminam', 'mammis'): 1.0,\n",
       " ('fencing', 'boxing'): 1.0,\n",
       " ('ferns', 'grasses'): 1.0,\n",
       " ('festivities', 'finer'): 1.0,\n",
       " ('fetid', 'closeness'): 1.0,\n",
       " ('filers', 'countersinkers'): 1.0,\n",
       " ('finical', 'criticism'): 1.0,\n",
       " ('fitz', 'swackhammer'): 1.0,\n",
       " ('fixes', 'distortions'): 1.0,\n",
       " ('flail', 'threatens'): 1.0,\n",
       " ('flinty', 'projections'): 1.0,\n",
       " ('flushed', 'conquest'): 1.0,\n",
       " ('foie', 'gras'): 1.0,\n",
       " ('forerunning', 'couriers'): 1.0,\n",
       " ('forges', 'melt'): 1.0,\n",
       " ('formally', 'indite'): 1.0,\n",
       " ('forswears', 'disbands'): 1.0,\n",
       " ('fort', 'cattegat'): 1.0,\n",
       " ('fray', 'elemental'): 1.0,\n",
       " ('freewill', 'discriminating'): 1.0,\n",
       " ('frequency', 'recur'): 1.0,\n",
       " ('freshness', 'genuineness'): 1.0,\n",
       " ('frugal', 'housekeepers'): 1.0,\n",
       " ('funereal', 'pyres'): 1.0,\n",
       " ('furred', 'hoar'): 1.0,\n",
       " ('gallopingly', 'reviewed'): 1.0,\n",
       " ('ganders', 'formally'): 1.0,\n",
       " ('gaseous', 'fata'): 1.0,\n",
       " ('gastric', 'juices'): 1.0,\n",
       " ('gazers', 'circumambulate'): 1.0,\n",
       " ('gazettes', 'extras'): 1.0,\n",
       " ('gem', 'wearer'): 1.0,\n",
       " ('genteel', 'comedies'): 1.0,\n",
       " ('ger', 'wallen'): 1.0,\n",
       " ('glarings', 'doltish'): 1.0,\n",
       " ('glim', 'jiffy'): 1.0,\n",
       " ('gloomiest', 'reserve'): 1.0,\n",
       " ('gobbles', 'bullets'): 1.0,\n",
       " ('godhead', 'hindoos'): 1.0,\n",
       " ('grains', 'claret'): 1.0,\n",
       " ('grated', 'nutmeg'): 1.0,\n",
       " ('graved', 'ahaz'): 1.0,\n",
       " ('greener', 'fresher'): 1.0,\n",
       " ('greybeards', 'oftenest'): 1.0,\n",
       " ('grocers', 'costermongers'): 1.0,\n",
       " ('growlands', 'walfish'): 1.0,\n",
       " ('guarding', 'protecting'): 1.0,\n",
       " ('gudgeon', 'retires'): 1.0,\n",
       " ('habeat', 'caput'): 1.0,\n",
       " ('habergeon', 'esteemeth'): 1.0,\n",
       " ('hacking', 'horrifying'): 1.0,\n",
       " ('hain', 'sittin'): 1.0,\n",
       " ('halfspent', 'suction'): 1.0,\n",
       " ('hallowed', 'precincts'): 1.0,\n",
       " ('ham', 'squattings'): 1.0,\n",
       " ('hangman', 'nooses'): 1.0,\n",
       " ('harvesting', 'hacking'): 1.0,\n",
       " ('hastier', 'withdrawals'): 1.0,\n",
       " ('hastily', 'slewing'): 1.0,\n",
       " ('haughtily', 'courteously'): 1.0,\n",
       " ('heedlessly', 'gambol'): 1.0,\n",
       " ('heir', 'overlording'): 1.0,\n",
       " ('herman', 'melville'): 1.0,\n",
       " ('hey', 'hey'): 1.0,\n",
       " ('hid', 'heliotrope'): 1.0,\n",
       " ('higgledy', 'piggledy'): 1.0,\n",
       " ('hillock', 'elbow'): 1.0,\n",
       " ('hollanders', 'zealanders'): 1.0,\n",
       " ('homeless', 'selves'): 1.0,\n",
       " ('hoods', 'ghent'): 1.0,\n",
       " ('horatii', 'pirouetting'): 1.0,\n",
       " ('hotel', 'cluny'): 1.0,\n",
       " ('houseless', 'familyless'): 1.0,\n",
       " ('howls', 'louder'): 1.0,\n",
       " ('hummingly', 'soliloquizes'): 1.0,\n",
       " ('hussar', 'surcoat'): 1.0,\n",
       " ('hwal', 'swedish'): 1.0,\n",
       " ('hydriote', 'canaris'): 1.0,\n",
       " ('hypochondriac', 'supine'): 1.0,\n",
       " ('ibis', 'roasted'): 1.0,\n",
       " ('idolatrous', 'dotings'): 1.0,\n",
       " ('illimitably', 'invaded'): 1.0,\n",
       " ('impalpable', 'destructive'): 1.0,\n",
       " ('imported', 'cobblestones'): 1.0,\n",
       " ('imposed', 'coarse'): 1.0,\n",
       " ('impotent', 'repentant'): 1.0,\n",
       " ('imps', 'blocksburg'): 1.0,\n",
       " ('incited', 'taunts'): 1.0,\n",
       " ('incoherences', 'uninvitedly'): 1.0,\n",
       " ('inconclusive', 'differences'): 1.0,\n",
       " ('incrustations', 'overgrowing'): 1.0,\n",
       " ('indistinctly', 'hesitatingly'): 1.0,\n",
       " ('indite', 'circulars'): 1.0,\n",
       " ('ineffectual', 'guarded'): 1.0,\n",
       " ('inexperience', 'incompetency'): 1.0,\n",
       " ('inferentially', 'negatived'): 1.0,\n",
       " ('infinity', 'firmest'): 1.0,\n",
       " ('infixed', 'unrelenting'): 1.0,\n",
       " ('ingeniously', 'overcome'): 1.0,\n",
       " ('inhospitable', 'wilds'): 1.0,\n",
       " ('inquiries', 'learnt'): 1.0,\n",
       " ('inquisition', 'wanes'): 1.0,\n",
       " ('inscribed', 'unsolved'): 1.0,\n",
       " ('inter', 'indebtedness'): 1.0,\n",
       " ('interferes', 'benevolence'): 1.0,\n",
       " ('interweave', 'antlers'): 1.0,\n",
       " ('intimacy', 'friendliness'): 1.0,\n",
       " ('intrantem', 'feminam'): 1.0,\n",
       " ('inventive', 'unscrupulous'): 1.0,\n",
       " ('inventor', 'patentee'): 1.0,\n",
       " ('inventors', 'patentees'): 1.0,\n",
       " ('invertedly', 'contradict'): 1.0,\n",
       " ('investigated', 'incongruity'): 1.0,\n",
       " ('inwreathing', 'orisons'): 1.0,\n",
       " ('ironical', 'coincidings'): 1.0,\n",
       " ('irresponsible', 'ferociousness'): 1.0,\n",
       " ('isabella', 'inquisition'): 1.0,\n",
       " ('isthmus', 'darien'): 1.0,\n",
       " ('jackson', 'pebbles'): 1.0,\n",
       " ('jambs', 'bricks'): 1.0,\n",
       " ('japonicas', 'pearls'): 1.0,\n",
       " ('jaundice', 'infirmity'): 1.0,\n",
       " ('jav', 'lins'): 1.0,\n",
       " ('jebb', 'anticipative'): 1.0,\n",
       " ('journal', 'tyerman'): 1.0,\n",
       " ('journeyman', 'joiner'): 1.0,\n",
       " ('juan', 'fernandes'): 1.0,\n",
       " ('jungle', 'overlays'): 1.0,\n",
       " ('jure', 'meritoque'): 1.0,\n",
       " ('kentucky', 'mammoth'): 1.0,\n",
       " ('kinross', 'myself'): 1.0,\n",
       " ('kit', 'carson'): 1.0,\n",
       " ('knights', 'squires'): 1.0,\n",
       " ('knob', 'slamming'): 1.0,\n",
       " ('knobby', 'ostrich'): 1.0,\n",
       " ('knocks', 'coke'): 1.0,\n",
       " ('knotty', 'aroostook'): 1.0,\n",
       " ('koo', 'loo'): 1.0,\n",
       " ('lamatins', 'dugongs'): 1.0,\n",
       " ('lament', 'parents'): 1.0,\n",
       " ('lanes', 'alleys'): 1.0,\n",
       " ('lassitude', 'overtakes'): 1.0,\n",
       " ('lege', 'naturae'): 1.0,\n",
       " ('legislative', 'enactment'): 1.0,\n",
       " ('lens', 'herschel'): 1.0,\n",
       " ('leopards', 'unrecking'): 1.0,\n",
       " ('lessons', 'inculcated'): 1.0,\n",
       " ('leuwenhoeck', 'submits'): 1.0,\n",
       " ('levanter', 'simoon'): 1.0,\n",
       " ('leviathanism', 'signifying'): 1.0,\n",
       " ('lightest', 'corky'): 1.0,\n",
       " ('limestone', 'marl'): 1.0,\n",
       " ('limitless', 'uncharted'): 1.0,\n",
       " ('limpid', 'odoriferous'): 1.0,\n",
       " ('lipless', 'unfeatured'): 1.0,\n",
       " ('listened', 'unhappy'): 1.0,\n",
       " ('liturgies', 'xxxix'): 1.0,\n",
       " ('loitering', 'shady'): 1.0,\n",
       " ('loiters', 'predicted'): 1.0,\n",
       " ('longed', 'vermillion'): 1.0,\n",
       " ('lotus', 'unfolding'): 1.0,\n",
       " ('loungingly', 'managed'): 1.0,\n",
       " ('lovings', 'longings'): 1.0,\n",
       " ('loyal', 'britons'): 1.0,\n",
       " ('lulled', 'opium'): 1.0,\n",
       " ('lunar', 'astral'): 1.0,\n",
       " ('luridly', 'illumined'): 1.0,\n",
       " ('luxuriant', 'profusion'): 1.0,\n",
       " ('maachah', 'judea'): 1.0,\n",
       " ('machine', 'automaton'): 1.0,\n",
       " ('magnify', 'archaeological'): 1.0,\n",
       " ('magniloquent', 'ascriptions'): 1.0,\n",
       " ('maidenly', 'gentleness'): 1.0,\n",
       " ('mammis', 'lactantem'): 1.0,\n",
       " ('manes', 'scowled'): 1.0,\n",
       " ('manfully', 'sheering'): 1.0,\n",
       " ('marius', 'sylla'): 1.0,\n",
       " ('marl', 'bequeathed'): 1.0,\n",
       " ('marsh', 'perpetuates'): 1.0,\n",
       " ('matched', 'overmanned'): 1.0,\n",
       " ('matse', 'avatar'): 1.0,\n",
       " ('matsmai', 'sikoke'): 1.0,\n",
       " ('meanly', 'contemptibly'): 1.0,\n",
       " ('medicament', 'druggists'): 1.0,\n",
       " ('meritoque', 'submitted'): 1.0,\n",
       " ('meshach', 'abednego'): 1.0,\n",
       " ('metempsychosis', 'pythagoras'): 1.0,\n",
       " ('methods', 'intelligently'): 1.0,\n",
       " ('michael', 'raphael'): 1.0,\n",
       " ('miller', 'shuts'): 1.0,\n",
       " ('minor', 'contingencies'): 1.0,\n",
       " ('mints', 'spanishly'): 1.0,\n",
       " ('miscellaneously', 'carnivorous'): 1.0,\n",
       " ('moaning', 'squadrons'): 1.0,\n",
       " ('mocks', 'dares'): 1.0,\n",
       " ('mohawk', 'counties'): 1.0,\n",
       " ('moidores', 'pistoles'): 1.0,\n",
       " ('monitions', 'unneeded'): 1.0,\n",
       " ('monks', 'dunfermline'): 1.0,\n",
       " ('monsoons', 'pampas'): 1.0,\n",
       " ('montaigne', 'apology'): 1.0,\n",
       " ('moorish', 'scimetars'): 1.0,\n",
       " ('morally', 'enfeebled'): 1.0,\n",
       " ('mote', 'availle'): 1.0,\n",
       " ('mountaineers', 'alleghanian'): 1.0,\n",
       " ('movingly', 'admonish'): 1.0,\n",
       " ('mufti', 'thronged'): 1.0,\n",
       " ('multum', 'parvo'): 1.0,\n",
       " ('mummeries', 'unmeaningly'): 1.0,\n",
       " ('mutes', 'bowstring'): 1.0,\n",
       " ('nailest', 'geese'): 1.0,\n",
       " ('naturae', 'jure'): 1.0,\n",
       " ('needing', 'supervision'): 1.0,\n",
       " ('neighbor', 'cholo'): 1.0,\n",
       " ('nets', 'mackerel'): 1.0,\n",
       " ('nibbling', 'goats'): 1.0,\n",
       " ('niphon', 'matsmai'): 1.0,\n",
       " ('nondescript', 'provincialisms'): 1.0,\n",
       " ('nosegays', 'damsels'): 1.0,\n",
       " ('nudging', 'tahitan'): 1.0,\n",
       " ('numbed', 'wasps'): 1.0,\n",
       " ('nun', 'evoke'): 1.0,\n",
       " ('oakes', 'smith'): 1.0,\n",
       " ('objected', 'reserving'): 1.0,\n",
       " ('observest', 'sashless'): 1.0,\n",
       " ('occult', 'lessons'): 1.0,\n",
       " ('occupant', 'occupants'): 1.0,\n",
       " ('octavoes', '.*--'): 1.0,\n",
       " ('odious', 'stigma'): 1.0,\n",
       " ('ointment', 'medicament'): 1.0,\n",
       " ('omnisciently', 'exhaustive'): 1.0,\n",
       " ('oppositely', 'voided'): 1.0,\n",
       " ('orion', 'glitters'): 1.0,\n",
       " ('orleans', 'whiskey'): 1.0,\n",
       " ('ornamental', 'knobs'): 1.0,\n",
       " ('outcries', 'anathemas'): 1.0,\n",
       " ('overburdening', 'panniers'): 1.0,\n",
       " ('overtakes', 'sated'): 1.0,\n",
       " ('painstaking', 'burrower'): 1.0,\n",
       " ('pannangians', 'brighggians'): 1.0,\n",
       " ('parisians', 'entrenched'): 1.0,\n",
       " ('partial', 'resemblances'): 1.0,\n",
       " ('partiality', 'undue'): 1.0,\n",
       " ('parvo', 'sheffield'): 1.0,\n",
       " ('pascal', 'rousseau'): 1.0,\n",
       " ('passively', 'await'): 1.0,\n",
       " ('pasteboard', 'masks'): 1.0,\n",
       " ('pate', 'foie'): 1.0,\n",
       " ('patris', 'sed'): 1.0,\n",
       " ('pattern', 'imprimis'): 1.0,\n",
       " ('pealing', 'exultation'): 1.0,\n",
       " ('pedestals', 'statues'): 1.0,\n",
       " ('pekee', 'nuee'): 1.0,\n",
       " ('peltry', 'wigwams'): 1.0,\n",
       " ('penal', 'gout'): 1.0,\n",
       " ('penem', 'intrantem'): 1.0,\n",
       " ('perfumery', 'pastiles'): 1.0,\n",
       " ('phil', 'trans'): 1.0,\n",
       " ('philosophically', 'drawled'): 1.0,\n",
       " ('pictorial', 'delusions'): 1.0,\n",
       " ('piggin', 'bailer'): 1.0,\n",
       " ('pilau', 'breadfruit'): 1.0,\n",
       " ('pinned', 'groove'): 1.0,\n",
       " ('piously', 'pounce'): 1.0,\n",
       " ('pitted', 'mutilated'): 1.0,\n",
       " ('plaintiveness', 'inwreathing'): 1.0,\n",
       " ('plausible', 'confirmation'): 1.0,\n",
       " ('plums', 'rubies'): 1.0,\n",
       " ('plutarch', 'morals'): 1.0,\n",
       " ('pointless', 'centres'): 1.0,\n",
       " ('poncho', 'slipt'): 1.0,\n",
       " ('postscript', 'behalf'): 1.0,\n",
       " ('pottowottamie', 'sachem'): 1.0,\n",
       " ('powders', 'pomatum'): 1.0,\n",
       " ('praetorians', 'auction'): 1.0,\n",
       " ('precedents', 'utility'): 1.0,\n",
       " ('precedes', 'prophesies'): 1.0,\n",
       " ('preciousness', 'enhancing'): 1.0,\n",
       " ('preface', 'gondibert'): 1.0,\n",
       " ('preparatives', 'needing'): 1.0,\n",
       " ('presaging', 'vibrations'): 1.0,\n",
       " ('proclaimed', 'inheritor'): 1.0,\n",
       " ('proffer', 'passer'): 1.0,\n",
       " ('proverbial', 'evanescence'): 1.0,\n",
       " ('provincial', 'sentimentalist'): 1.0,\n",
       " ('ptolemy', 'philopater'): 1.0,\n",
       " ('purposed', 'befriending'): 1.0,\n",
       " ('quarrel', 'backwoods'): 1.0,\n",
       " ('quenchless', 'feud'): 1.0,\n",
       " ('quietest', 'enchanting'): 1.0,\n",
       " ('ragamuffin', 'rapscallions'): 1.0,\n",
       " ('raimond', 'sebond'): 1.0,\n",
       " ('ramifying', 'heartlessness'): 1.0,\n",
       " ('randolphs', 'hardicanutes'): 1.0,\n",
       " ('ravished', 'europa'): 1.0,\n",
       " ('raw', 'recruit'): 1.0,\n",
       " ('reddenest', 'palest'): 1.0,\n",
       " ('redolent', 'myrrh'): 1.0,\n",
       " ('reg', 'lar'): 1.0,\n",
       " ('regina', 'caudam'): 1.0,\n",
       " ('regulating', 'circulation'): 1.0,\n",
       " ('remonstrate', 'silenced'): 1.0,\n",
       " ('rending', 'goring'): 1.0,\n",
       " ('renewed', 'onward'): 1.0,\n",
       " ('rensselaers', 'randolphs'): 1.0,\n",
       " ('repentant', 'admonitory'): 1.0,\n",
       " ('replenishes', 'mugs'): 1.0,\n",
       " ('repulses', 'accumulating'): 1.0,\n",
       " ('residuary', 'legatees'): 1.0,\n",
       " ('restraint', 'tipping'): 1.0,\n",
       " ('returne', 'againe'): 1.0,\n",
       " ('revelled', 'dalliance'): 1.0,\n",
       " ('rex', 'habeat'): 1.0,\n",
       " ('reydan', 'siskur'): 1.0,\n",
       " ('rigadig', 'tunes'): 1.0,\n",
       " ('rigorous', 'discipline'): 1.0,\n",
       " ('rinaldo', 'rinaldini'): 1.0,\n",
       " ('riotously', 'perverse'): 1.0,\n",
       " ('ripening', 'apricot'): 1.0,\n",
       " ('rivallingly', 'discolour'): 1.0,\n",
       " ('roly', 'poly'): 1.0,\n",
       " ('rondeletius', 'willoughby'): 1.0,\n",
       " ('roofs', 'domes'): 1.0,\n",
       " ('routine', 'metempsychosis'): 1.0,\n",
       " ('rue', 'dauphine'): 1.0,\n",
       " ('ruffed', 'mendanna'): 1.0,\n",
       " ('rug', 'softest'): 1.0,\n",
       " ('rumbles', 'talks'): 1.0,\n",
       " ('ruptured', 'membranes'): 1.0,\n",
       " ('rustlings', 'festooned'): 1.0,\n",
       " ('saints', 'demigods'): 1.0,\n",
       " ('same', 'cocked'): 1.0,\n",
       " ('samphire', 'baskets'): 1.0,\n",
       " ('sanctuary', 'wastingly'): 1.0,\n",
       " ('sanctum', 'sanctorum'): 1.0,\n",
       " ('saplings', 'mimicking'): 1.0,\n",
       " ('saul', 'tarsus'): 1.0,\n",
       " ('scheming', 'unappeasedly'): 1.0,\n",
       " ('schmerenburgh', 'smeerenberg'): 1.0,\n",
       " ('scimetars', 'scabbards'): 1.0,\n",
       " ('scolds', 'lesser'): 1.0,\n",
       " ('scornfully', 'champed'): 1.0,\n",
       " ('scorning', 'turnstile'): 1.0,\n",
       " ('scorpio', 'scorpion'): 1.0,\n",
       " ('scorpion', 'stings'): 1.0,\n",
       " ('scrabble', 'scramble'): 1.0,\n",
       " ('scraggy', 'scoria'): 1.0,\n",
       " ('seconds', 'tick'): 1.0,\n",
       " ('seemly', 'correspondence'): 1.0,\n",
       " ('selectest', 'champions'): 1.0,\n",
       " ('seminal', 'germs'): 1.0,\n",
       " ('seneca', 'stoics'): 1.0,\n",
       " ('sentence', 'hobbes'): 1.0,\n",
       " ('sequential', 'issues'): 1.0,\n",
       " ('serfs', 'republican'): 1.0,\n",
       " ('serpentine', 'spiralise'): 1.0,\n",
       " ('settlements', 'harems'): 1.0,\n",
       " ('shadiest', 'quietest'): 1.0,\n",
       " ('shadrach', 'meshach'): 1.0,\n",
       " ('shagginess', 'episode'): 1.0,\n",
       " ('shakespeare', 'melancthon'): 1.0,\n",
       " ('shallowest', 'assumption'): 1.0,\n",
       " ('shave', 'sup'): 1.0,\n",
       " ('shields', 'medallions'): 1.0,\n",
       " ('shiftings', 'windrowed'): 1.0,\n",
       " ('shingled', 'attic'): 1.0,\n",
       " ('shrinked', 'sheered'): 1.0,\n",
       " ('shrubs', 'ferns'): 1.0,\n",
       " ('shuts', 'watergate'): 1.0,\n",
       " ('shutters', 'footman'): 1.0,\n",
       " ('sinning', 'sinned'): 1.0,\n",
       " ('skittishly', 'curvetting'): 1.0,\n",
       " ('slanderous', 'aspersion'): 1.0,\n",
       " ('slatternly', 'untidy'): 1.0,\n",
       " ('sleepiest', 'sunsets'): 1.0,\n",
       " ('slicings', 'severs'): 1.0,\n",
       " ('slipperiness', 'curb'): 1.0,\n",
       " ('slippering', 'misbehaviour'): 1.0,\n",
       " ('smackingly', 'feasted'): 1.0,\n",
       " ('smothering', 'conflagration'): 1.0,\n",
       " ('smugglers', 'contraband'): 1.0,\n",
       " ('smuggling', 'verbalists'): 1.0,\n",
       " ('snakes', 'sportively'): 1.0,\n",
       " ('soberly', 'recurred'): 1.0,\n",
       " ('softest', 'turkey'): 1.0,\n",
       " ('sog', 'sogger'): 1.0,\n",
       " ('soggy', 'squitchy'): 1.0,\n",
       " ('solaces', 'endearments'): 1.0,\n",
       " ('sourceless', 'primogenitures'): 1.0,\n",
       " ('specialities', 'concentrations'): 1.0,\n",
       " ('spells', 'potencies'): 1.0,\n",
       " ('spencer', 'talus'): 1.0,\n",
       " ('spicin', \"',--\"): 1.0,\n",
       " ('spiked', 'hotel'): 1.0,\n",
       " ('spiracles', 'apertures'): 1.0,\n",
       " ('spiritually', 'feasting'): 1.0,\n",
       " ('spleen', 'regulating'): 1.0,\n",
       " ('spontaneous', 'literal'): 1.0,\n",
       " ('sportively', 'festooning'): 1.0,\n",
       " ('spurrings', 'goadings'): 1.0,\n",
       " ('squadrons', 'asphaltites'): 1.0,\n",
       " ('stalwart', 'frames'): 1.0,\n",
       " ('statues', 'shields'): 1.0,\n",
       " ('stifle', 'upbraidings'): 1.0,\n",
       " ('stig', 'quig'): 1.0,\n",
       " ('stigma', 'originate'): 1.0,\n",
       " ('strenuous', 'exertions'): 1.0,\n",
       " ('stubble', 'laugheth'): 1.0,\n",
       " ('submission', 'endurance'): 1.0,\n",
       " ('submits', 'inspection'): 1.0,\n",
       " ('subordinately', 'emblazoned'): 1.0,\n",
       " ('subscribes', 'durand'): 1.0,\n",
       " ('substantive', 'deformity'): 1.0,\n",
       " ('subtly', 'malignantly'): 1.0,\n",
       " ('sufficit', 'rex'): 1.0,\n",
       " ('suicide', 'contributed'): 1.0,\n",
       " ('surly', 'dabs'): 1.0,\n",
       " ('surrenderest', 'hypo'): 1.0,\n",
       " ('swagger', 'unshunned'): 1.0,\n",
       " ('swayings', 'coyings'): 1.0,\n",
       " ('sweepers', 'billeted'): 1.0,\n",
       " ('sweetener', 'softener'): 1.0,\n",
       " ('sylla', 'classic'): 1.0,\n",
       " ('tee', 'twisk'): 1.0,\n",
       " ('tekel', 'upharsin'): 1.0,\n",
       " ('tenpin', 'andirons'): 1.0,\n",
       " ('terribly', 'infected'): 1.0,\n",
       " ('thief', 'catcher'): 1.0,\n",
       " ('thorkill', 'hake'): 1.0,\n",
       " ('thro', 'maine'): 1.0,\n",
       " ('throned', 'torsoes'): 1.0,\n",
       " ('thunderous', 'concussion'): 1.0,\n",
       " ('tiara', 'ewer'): 1.0,\n",
       " ('tic', 'dolly'): 1.0,\n",
       " ('tick', 'immaterial'): 1.0,\n",
       " ('ties', 'connexions'): 1.0,\n",
       " ('tinkerings', 'betters'): 1.0,\n",
       " ('tint', 'bestreaked'): 1.0,\n",
       " ('toils', 'trials'): 1.0,\n",
       " ('tongatobooarrs', 'erromanggoans'): 1.0,\n",
       " ('tooke', 'lucian'): 1.0,\n",
       " ('toothpick', 'rayther'): 1.0,\n",
       " ('torpid', 'intellects'): 1.0,\n",
       " ('torso', 'deceased'): 1.0,\n",
       " ('trained', 'chargers'): 1.0,\n",
       " ('trans', '1668'): 1.0,\n",
       " ('transactions', 'relate'): 1.0,\n",
       " ('transcendental', 'platonic'): 1.0,\n",
       " ('triangularly', 'platformed'): 1.0,\n",
       " ('trio', 'lancers'): 1.0,\n",
       " ('trover', 'litigated'): 1.0,\n",
       " ('tudors', 'bourbons'): 1.0,\n",
       " ('tutelary', 'guardian'): 1.0,\n",
       " ('twigs', 'productive'): 1.0,\n",
       " ('twiske', 'tee'): 1.0,\n",
       " ('tyre', 'carthage'): 1.0,\n",
       " ('ugliest', 'abortion'): 1.0,\n",
       " ('unachieved', 'revengeful'): 1.0,\n",
       " ('unassured', 'deprecating'): 1.0,\n",
       " ('unbiased', 'freewill'): 1.0,\n",
       " ('unbuckling', 'garters'): 1.0,\n",
       " ('uncanonical', 'rabbins'): 1.0,\n",
       " ('unceremoniously', 'bagged'): 1.0,\n",
       " ('unconditionally', 'reiterating'): 1.0,\n",
       " ('uncontaminated', 'aroma'): 1.0,\n",
       " ('undetected', 'villain'): 1.0,\n",
       " ('unequal', 'crosslights'): 1.0,\n",
       " ('unflattering', 'laureate'): 1.0,\n",
       " ('unhesitatingly', 'expert'): 1.0,\n",
       " ('unicorns', 'infesting'): 1.0,\n",
       " ('unintegral', 'mastery'): 1.0,\n",
       " ('unlock', 'bridegroom'): 1.0,\n",
       " ('unmanageably', 'winces'): 1.0,\n",
       " ('unmisgiving', 'hardihood'): 1.0,\n",
       " ('unmurmuringly', 'acquiesced'): 1.0,\n",
       " ('unnamable', 'imminglings'): 1.0,\n",
       " ('unpoetical', 'disreputable'): 1.0,\n",
       " ('unrecking', 'unworshipping'): 1.0,\n",
       " ('unsignifying', 'pettiness'): 1.0,\n",
       " ('unsocial', 'repelling'): 1.0,\n",
       " ('unstirring', 'paralysis'): 1.0,\n",
       " ('unsuccessful', 'onsets'): 1.0,\n",
       " ('unsurrenderable', 'wilfulness'): 1.0,\n",
       " ('untraceable', 'evolutions'): 1.0,\n",
       " ('untraditionally', 'independently'): 1.0,\n",
       " ('untrodden', 'unwilted'): 1.0,\n",
       " ('unwelcome', 'truths'): 1.0,\n",
       " ('vagueness', 'painfulness'): 1.0,\n",
       " ('vero', 'sufficit'): 1.0,\n",
       " ('versailles', 'beholder'): 1.0,\n",
       " ('vesper', 'hymns'): 1.0,\n",
       " ('vignettes', 'embellishments'): 1.0,\n",
       " ('villainies', 'detected'): 1.0,\n",
       " ('vineyards', 'champagne'): 1.0,\n",
       " ('vintage', 'vineyards'): 1.0,\n",
       " ('viol', 'spiracles'): 1.0,\n",
       " ('violate', 'pythagorean'): 1.0,\n",
       " ('virtuous', 'elder'): 1.0,\n",
       " ('vitus', 'imp'): 1.0,\n",
       " ('vividness', 'scorches'): 1.0,\n",
       " ('volatile', 'salts'): 1.0,\n",
       " ('voluntary', 'confiding'): 1.0,\n",
       " ('waded', 'nets'): 1.0,\n",
       " ('wainscots', 'reminding'): 1.0,\n",
       " ('waive', 'ceremonial'): 1.0,\n",
       " ('walfish', 'swedes'): 1.0,\n",
       " ('wallen', 'walw'): 1.0,\n",
       " ('walter', 'canny'): 1.0,\n",
       " ('walw', 'ian'): 1.0,\n",
       " ('wandered', 'eddied'): 1.0,\n",
       " ('wantonness', 'fuzzing'): 1.0,\n",
       " ('warbled', 'praises'): 1.0,\n",
       " ('warbling', 'persuasiveness'): 1.0,\n",
       " ('warmer', 'borneo'): 1.0,\n",
       " ('warringly', 'interweave'): 1.0,\n",
       " ('watcher', 'dietetically'): 1.0,\n",
       " ('wealthiest', 'praetorians'): 1.0,\n",
       " ('wery', 'woracious'): 1.0,\n",
       " ('wester', 'bombazine'): 1.0,\n",
       " ('westers', 'harmattans'): 1.0,\n",
       " ('whelmings', 'intermixingly'): 1.0,\n",
       " ('whisperingly', 'urging'): 1.0,\n",
       " ('whoel', 'anglo'): 1.0,\n",
       " ('whooping', 'imps'): 1.0,\n",
       " ('willis', 'ellery'): 1.0,\n",
       " ('wills', 'testaments'): 1.0,\n",
       " ('winces', 'unconcluded'): 1.0,\n",
       " ('wines', 'rhenish'): 1.0,\n",
       " ('wiping', 'profuse'): 1.0,\n",
       " ('witch', 'copenhagen'): 1.0,\n",
       " ('wonst', 'cibil'): 1.0,\n",
       " ('woo', 'hoo'): 1.0,\n",
       " ('worldly', 'ties'): 1.0,\n",
       " ('worming', 'undulation'): 1.0,\n",
       " ('wounds', 'bleed'): 1.0,\n",
       " ('wrangling', 'scuffling'): 1.0,\n",
       " ('wrapall', 'dreadnaught'): 1.0,\n",
       " ('zig', 'zag'): 1.0,\n",
       " ('fluke', 'chains'): 0.9920634920634921,\n",
       " ('martha', 'vineyard'): 0.9642857142857143,\n",
       " ('steering', 'oar'): 0.9423076923076923,\n",
       " ('seven', 'hundred'): 0.91125,\n",
       " ('hither', 'thither'): 0.8928571428571429,\n",
       " ('epaulets', 'epaulets'): 0.8888888888888888,\n",
       " ('inclined', 'plane'): 0.8888888888888888,\n",
       " ('marling', 'spike'): 0.8888888888888888,\n",
       " ('massachusetts', 'calendar'): 0.8888888888888888,\n",
       " ('obed', 'macy'): 0.8888888888888888,\n",
       " ('riding', 'whips'): 0.8888888888888888,\n",
       " ('smoothe', 'seam'): 0.8888888888888888,\n",
       " ('sneezes', ')--'): 0.84375,\n",
       " ('thou', 'art'): 0.8334983349833498,\n",
       " ('loose', 'fish'): 0.8306001690617075,\n",
       " ('look', 'outs'): 0.824390243902439,\n",
       " ('huzza', 'porpoise'): 0.8223684210526315,\n",
       " ('admirable', 'artistic'): 0.8,\n",
       " ('cousin', 'hosea'): 0.8,\n",
       " ('cows', 'calves'): 0.8,\n",
       " ('furthest', 'bounds'): 0.8,\n",
       " ('harris', 'coll'): 0.8,\n",
       " ('leathern', 'tally'): 0.8,\n",
       " ('reverend', 'clergy'): 0.8,\n",
       " ('tier', 'butts'): 0.8,\n",
       " ('centuries', 'ago'): 0.7939814814814815,\n",
       " ('good', 'bye'): 0.7824074074074074,\n",
       " ('beef', 'bread'): 0.75,\n",
       " ('fiery', 'pit'): 0.7440476190476191,\n",
       " ('rose', 'bud'): 0.7142857142857143,\n",
       " ('kings', 'queens'): 0.7111111111111111,\n",
       " ('king', 'post'): 0.7102272727272727,\n",
       " ('cutting', 'spade'): 0.6918918918918919,\n",
       " ('ivory', 'leg'): 0.6771669341894061,\n",
       " ('arrayed', 'decent'): 0.6666666666666666,\n",
       " ('fore', 'aft'): 0.6666666666666666,\n",
       " ('ply', 'shuttle'): 0.6666666666666666,\n",
       " ('reef', 'topsails'): 0.6666666666666666,\n",
       " ('speechless', 'placeless'): 0.6666666666666666,\n",
       " ('wrists', 'ankles'): 0.6666666666666666,\n",
       " ('horse', 'shoe'): 0.6613756613756614,\n",
       " ('battering', 'ram'): 0.6428571428571429,\n",
       " ('congregational', 'church'): 0.6428571428571429,\n",
       " ('sumatra', 'java'): 0.6428571428571429,\n",
       " ('never', 'mind'): 0.6381397578808582,\n",
       " ('latitudes', 'longitudes'): 0.6,\n",
       " ('life', 'buoy'): 0.5884173297966402,\n",
       " ('brazil', 'banks'): 0.5714285714285714,\n",
       " ('caulk', 'seams'): 0.5714285714285714,\n",
       " ('dan', 'coopman'): 0.5714285714285714,\n",
       " ('dilated', 'nostrils'): 0.5714285714285714,\n",
       " ('gentle', 'globules'): 0.5714285714285714,\n",
       " ('grotesque', 'figures'): 0.5714285714285714,\n",
       " ('kindled', 'shavings'): 0.5714285714285714,\n",
       " ('sub', 'sub'): 0.5510204081632653,\n",
       " ('forty', 'years'): 0.5454545454545454,\n",
       " ('pine', 'tree'): 0.5411255411255411,\n",
       " ('full', 'grown'): 0.5410569105691057,\n",
       " ('moth', 'rust'): 0.5333333333333333,\n",
       " ('plum', 'pudding'): 0.5333333333333333,\n",
       " ('robust', 'healthy'): 0.5333333333333333,\n",
       " ('tablet', 'erected'): 0.5333333333333333,\n",
       " ('bashee', 'isles'): 0.5294117647058824,\n",
       " ('gay', 'header'): 0.5079365079365079,\n",
       " ('.\"*', 'nominal'): 0.5,\n",
       " ('1779', 'disinterred'): 0.5,\n",
       " ('21st', 'june'): 0.5,\n",
       " ('abed', 'unendurable'): 0.5,\n",
       " ('abjectly', 'reduced'): 0.5,\n",
       " ('abomination', 'brook'): 0.5,\n",
       " ('abundantly', 'picturesquely'): 0.5,\n",
       " ('accumulate', 'princely'): 0.5,\n",
       " ('accumulating', 'piling'): 0.5,\n",
       " ('accurately', 'foretell'): 0.5,\n",
       " ('actively', 'engaged'): 0.5,\n",
       " ('adjacent', 'interdicted'): 0.5,\n",
       " ('admirably', 'satirical'): 0.5,\n",
       " ('affords', 'fewer'): 0.5,\n",
       " ('afire', 'drenching'): 0.5,\n",
       " ('agile', 'obstetrics'): 0.5,\n",
       " ('aglow', 'bridegrooms'): 0.5,\n",
       " ('agonized', 'respirations'): 0.5,\n",
       " ('aldermen', 'patent'): 0.5,\n",
       " ('alfred', '890'): 0.5,\n",
       " ('allowance', 'exclusive'): 0.5,\n",
       " ('alpacas', 'volcanoes'): 0.5,\n",
       " ('alternate', 'fitful'): 0.5,\n",
       " ('ambergriese', 'paunch'): 0.5,\n",
       " ('amplified', 'fortifications'): 0.5,\n",
       " ('amusing', 'pluck'): 0.5,\n",
       " ('ancientest', 'draughts'): 0.5,\n",
       " ('angelo', 'paints'): 0.5,\n",
       " ('anglo', 'saxon'): 0.5,\n",
       " ('animation', 'unexpected'): 0.5,\n",
       " ('ankers', 'geneva'): 0.5,\n",
       " ('anne', 'forthing'): 0.5,\n",
       " ('anoint', 'machinery'): 0.5,\n",
       " ('antiquities', 'throned'): 0.5,\n",
       " ('apostolic', 'lancer'): 0.5,\n",
       " ('appallingly', 'astonishing'): 0.5,\n",
       " ('arbitrary', 'vein'): 0.5,\n",
       " ('archaeological', 'fossiliferous'): 0.5,\n",
       " ('archbishop', 'savesoul'): 0.5,\n",
       " ('archer', 'amusing'): 0.5,\n",
       " ('arches', 'gateways'): 0.5,\n",
       " ('arching', 'segment'): 0.5,\n",
       " ('architect', 'builder'): 0.5,\n",
       " ('artificially', 'upheld'): 0.5,\n",
       " ('aspirations', 'prematurely'): 0.5,\n",
       " ('aspiring', 'rainbowed'): 0.5,\n",
       " ('assail', 'fatally'): 0.5,\n",
       " ('assailants', 'divinity'): 0.5,\n",
       " ('assailed', 'yells'): 0.5,\n",
       " ...}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder = BigramCollocationFinder.from_words(words)\n",
    "bgm = BigramAssocMeasures ()\n",
    "collocations = {bigram: pmi for bigram , pmi in finder.score_ngrams(bgm.mi_like)}\n",
    "collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9ff497",
   "metadata": {},
   "source": [
    "## Bag of Words-Darstellung von Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478dd801",
   "metadata": {},
   "source": [
    "*Bag of Words* ist eine Form der Textdarstellung, die eine mathematische Weiterverarbeitung erlaubt. Für den zu analysierden Textkorpus werden die darin vorkommenden Wörter aufgelistet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "666a5195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i've been 2 times to new york in 2011, but did not have the constitution for it.\", \"it didn't appeal to me.\", 'i preferred los angeles.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d81f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = 'word') #analyzer = 'word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c5079f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58fee933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 20)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 11)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 13)\t1\n",
      "  (2, 16)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4812a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_X = X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "def4d760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1]\n",
      " [0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(dense_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d3af7d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2011', 'angeles', 'appeal', 'been', 'but', 'constitution', 'did', 'didn', 'for', 'have', 'in', 'it', 'los', 'me', 'new', 'not', 'preferred', 'the', 'times', 'to', 've', 'york']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5706b4",
   "metadata": {},
   "source": [
    "## n-gram Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee00a30",
   "metadata": {},
   "source": [
    "Wörter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "59c46765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (1, 3), stop_words = 'english') #min_df = 10, max_df = 0.75,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dfa334b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f2fbe427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 21)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 24)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 25)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 9)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 16)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 17)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ed134186",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_X = X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "85173e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(dense_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "67c863a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2011', '2011 did', '2011 did constitution', 'angeles', 'appeal', 'constitution', 'did', 'did constitution', 'didn', 'didn appeal', 'los', 'los angeles', 'new', 'new york', 'new york 2011', 'preferred', 'preferred los', 'preferred los angeles', 'times', 'times new', 'times new york', 've', 've times', 've times new', 'york', 'york 2011', 'york 2011 did']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eef9a6",
   "metadata": {},
   "source": [
    "Zeichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "beb21025",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_ngrams = CountVectorizer(analyzer = 'char', ngram_range = (2, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d56c3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = char_ngrams.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "70ccbee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 284)\t1\n",
      "  (0, 103)\t1\n",
      "  (0, 528)\t2\n",
      "  (0, 195)\t3\n",
      "  (0, 18)\t2\n",
      "  (0, 158)\t1\n",
      "  (0, 219)\t1\n",
      "  (0, 233)\t1\n",
      "  (0, 347)\t3\n",
      "  (0, 0)\t2\n",
      "  (0, 128)\t1\n",
      "  (0, 78)\t3\n",
      "  (0, 493)\t3\n",
      "  (0, 298)\t1\n",
      "  (0, 341)\t1\n",
      "  (0, 243)\t1\n",
      "  (0, 458)\t1\n",
      "  (0, 506)\t1\n",
      "  (0, 384)\t1\n",
      "  (0, 64)\t2\n",
      "  (0, 364)\t1\n",
      "  (0, 249)\t1\n",
      "  (0, 536)\t1\n",
      "  (0, 93)\t1\n",
      "  (0, 541)\t1\n",
      "  :\t:\n",
      "  (2, 461)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 146)\t1\n",
      "  (2, 372)\t1\n",
      "  (2, 267)\t1\n",
      "  (2, 232)\t1\n",
      "  (2, 283)\t1\n",
      "  (2, 77)\t1\n",
      "  (2, 434)\t1\n",
      "  (2, 447)\t1\n",
      "  (2, 228)\t1\n",
      "  (2, 258)\t1\n",
      "  (2, 242)\t1\n",
      "  (2, 457)\t1\n",
      "  (2, 443)\t1\n",
      "  (2, 218)\t1\n",
      "  (2, 177)\t1\n",
      "  (2, 60)\t1\n",
      "  (2, 340)\t1\n",
      "  (2, 414)\t1\n",
      "  (2, 462)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 147)\t1\n",
      "  (2, 373)\t1\n",
      "  (2, 268)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "792db199",
   "metadata": {},
   "outputs": [],
   "source": [
    "denseX = X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d6e32a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1 ... 1 1 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(denseX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "99525a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 2', ' 2 ', ' 2 t', ' 2 ti', ' 2 tim', ' 20', ' 201', ' 2011', ' 2011,', ' a', ' an', ' ang', ' ange', ' angel', ' ap', ' app', ' appe', ' appea', ' b', ' be', ' bee', ' been', ' been ', ' bu', ' but', ' but ', ' but d', ' c', ' co', ' con', ' cons', ' const', ' d', ' di', ' did', ' did ', ' did n', ' didn', \" didn'\", ' f', ' fo', ' for', ' for ', ' for i', ' h', ' ha', ' hav', ' have', ' have ', ' i', ' in', ' in ', ' in 2', ' in 20', ' it', ' it.', ' l', ' lo', ' los', ' los ', ' los a', ' m', ' me', ' me.', ' n', ' ne', ' new', ' new ', ' new y', ' no', ' not', ' not ', ' not h', ' p', ' pr', ' pre', ' pref', ' prefe', ' t', ' th', ' the', ' the ', ' the c', ' ti', ' tim', ' time', ' times', ' to', ' to ', ' to m', ' to me', ' to n', ' to ne', ' y', ' yo', ' yor', ' york', ' york ', \"'t\", \"'t \", \"'t a\", \"'t ap\", \"'t app\", \"'v\", \"'ve\", \"'ve \", \"'ve b\", \"'ve be\", ', ', ', b', ', bu', ', but', ', but ', '01', '011', '011,', '011, ', '011, b', '1,', '1, ', '1, b', '1, bu', '1, but', '11', '11,', '11, ', '11, b', '11, bu', '2 ', '2 t', '2 ti', '2 tim', '2 time', '20', '201', '2011', '2011,', '2011, ', 'al', 'al ', 'al t', 'al to', 'al to ', 'an', 'ang', 'ange', 'angel', 'angele', 'ap', 'app', 'appe', 'appea', 'appeal', 'av', 'ave', 'ave ', 'ave t', 'ave th', 'be', 'bee', 'been', 'been ', 'been 2', 'bu', 'but', 'but ', 'but d', 'but di', 'co', 'con', 'cons', 'const', 'consti', 'd ', 'd l', 'd lo', 'd los', 'd los ', 'd n', 'd no', 'd not', 'd not ', 'di', 'did', 'did ', 'did n', 'did no', 'didn', \"didn'\", \"didn't\", 'dn', \"dn'\", \"dn't\", \"dn't \", \"dn't a\", 'e ', 'e b', 'e be', 'e bee', 'e been', 'e c', 'e co', 'e con', 'e cons', 'e t', 'e th', 'e the', 'e the ', 'e.', 'ea', 'eal', 'eal ', 'eal t', 'eal to', 'ed', 'ed ', 'ed l', 'ed lo', 'ed los', 'ee', 'een', 'een ', 'een 2', 'een 2 ', 'ef', 'efe', 'efer', 'eferr', 'eferre', 'el', 'ele', 'eles', 'eles.', 'en', 'en ', 'en 2', 'en 2 ', 'en 2 t', 'er', 'err', 'erre', 'erred', 'erred ', 'es', 'es ', 'es t', 'es to', 'es to ', 'es.', 'ew', 'ew ', 'ew y', 'ew yo', 'ew yor', 'fe', 'fer', 'ferr', 'ferre', 'ferred', 'fo', 'for', 'for ', 'for i', 'for it', 'ge', 'gel', 'gele', 'geles', 'geles.', 'ha', 'hav', 'have', 'have ', 'have t', 'he', 'he ', 'he c', 'he co', 'he con', 'i ', 'i p', 'i pr', 'i pre', 'i pref', \"i'\", \"i'v\", \"i've\", \"i've \", \"i've b\", 'id', 'id ', 'id n', 'id no', 'id not', 'idn', \"idn'\", \"idn't\", \"idn't \", 'im', 'ime', 'imes', 'imes ', 'imes t', 'in', 'in ', 'in 2', 'in 20', 'in 201', 'io', 'ion', 'ion ', 'ion f', 'ion fo', 'it', 'it ', 'it d', 'it di', 'it did', 'it.', 'itu', 'itut', 'ituti', 'itutio', 'k ', 'k i', 'k in', 'k in ', 'k in 2', 'l ', 'l t', 'l to', 'l to ', 'l to m', 'le', 'les', 'les.', 'lo', 'los', 'los ', 'los a', 'los an', 'me', 'me.', 'mes', 'mes ', 'mes t', 'mes to', 'n ', 'n 2', 'n 2 ', 'n 2 t', 'n 2 ti', 'n 20', 'n 201', 'n 2011', 'n f', 'n fo', 'n for', 'n for ', \"n'\", \"n't\", \"n't \", \"n't a\", \"n't ap\", 'ne', 'new', 'new ', 'new y', 'new yo', 'ng', 'nge', 'ngel', 'ngele', 'ngeles', 'no', 'not', 'not ', 'not h', 'not ha', 'ns', 'nst', 'nsti', 'nstit', 'nstitu', 'o ', 'o m', 'o me', 'o me.', 'o n', 'o ne', 'o new', 'o new ', 'on', 'on ', 'on f', 'on fo', 'on for', 'ons', 'onst', 'onsti', 'onstit', 'or', 'or ', 'or i', 'or it', 'or it.', 'ork', 'ork ', 'ork i', 'ork in', 'os', 'os ', 'os a', 'os an', 'os ang', 'ot', 'ot ', 'ot h', 'ot ha', 'ot hav', 'pe', 'pea', 'peal', 'peal ', 'peal t', 'pp', 'ppe', 'ppea', 'ppeal', 'ppeal ', 'pr', 'pre', 'pref', 'prefe', 'prefer', 'r ', 'r i', 'r it', 'r it.', 're', 'red', 'red ', 'red l', 'red lo', 'ref', 'refe', 'refer', 'referr', 'rk', 'rk ', 'rk i', 'rk in', 'rk in ', 'rr', 'rre', 'rred', 'rred ', 'rred l', 's ', 's a', 's an', 's ang', 's ange', 's t', 's to', 's to ', 's to n', 's.', 'st', 'sti', 'stit', 'stitu', 'stitut', 't ', 't a', 't ap', 't app', 't appe', 't d', 't di', 't did', 't did ', 't didn', 't h', 't ha', 't hav', 't have', 't.', 'th', 'the', 'the ', 'the c', 'the co', 'ti', 'tim', 'time', 'times', 'times ', 'tio', 'tion', 'tion ', 'tion f', 'tit', 'titu', 'titut', 'tituti', 'to', 'to ', 'to m', 'to me', 'to me.', 'to n', 'to ne', 'to new', 'tu', 'tut', 'tuti', 'tutio', 'tution', 'ut', 'ut ', 'ut d', 'ut di', 'ut did', 'uti', 'utio', 'ution', 'ution ', 've', 've ', 've b', 've be', 've bee', 've t', 've th', 've the', 'w ', 'w y', 'w yo', 'w yor', 'w york', 'yo', 'yor', 'york', 'york ', 'york i']\n"
     ]
    }
   ],
   "source": [
    "print(char_ngrams.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ad4b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
